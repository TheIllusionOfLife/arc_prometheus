# Development Plan: Complete AI Civilization (Phase 3) + Experimental Variations

**Date:** October 31, 2025
**Status:** Active Development
**Competition Deadline:** November 3, 2025

## Core Vision: Build the AI Civilization First

**Philosophy:** Validate that multi-agent evolution with crossover can solve ARC tasks. Competition score is a metric, not the goal.

**Current Status:**
- ✅ Phase 1 Complete: Crucible (data, sandbox, evaluation)
- ✅ Phase 2 Complete: Evolution (Programmer, Refiner, fitness, evolution loop)
- ✅ Phase 2.5 Complete: pass@2 submission format
- ⏳ Phase 3 Needed: Analyst, Tagger, Crossover, Population Dynamics

---

## Phase 3: Complete the AI Civilization (Days 1-3, 20-24 hours)

### Goal: Implement the missing agents to realize the full vision

---

### **Task 3.1: Analyst Agent** (Day 1, 6-8 hours)

**Purpose:** Separate pattern understanding from code generation

**What Analyst Does:**
1. Analyzes input-output examples
2. Infers abstract transformation rules (natural language)
3. Generates specification for Programmer
4. **Key insight:** Abstracts reasoning from implementation

**Implementation:**

```python
# src/arc_prometheus/cognitive_cells/analyst.py

class Analyst:
    """Analyzes ARC tasks to infer transformation rules."""

    def analyze_task(self, task_json) -> AnalysisResult:
        """
        Analyze training examples and infer the transformation pattern.

        Returns:
            AnalysisResult with:
            - pattern_description: Natural language rule
            - key_observations: List of important features
            - suggested_approach: High-level implementation strategy
            - confidence: How certain the analysis is
        """
        # 1. Format task for LLM
        prompt = self._create_analysis_prompt(task_json)

        # 2. Call Gemini for pattern analysis
        response = self.llm.generate(prompt, temperature=0.3)

        # 3. Parse structured analysis
        analysis = self._parse_analysis(response)

        return analysis

    def _create_analysis_prompt(self, task_json):
        return f"""Analyze this ARC task and infer the transformation rule.

Training examples:
{format_training_examples(task_json["train"])}

Your task:
1. Observe patterns across all training examples
2. Identify the core transformation rule
3. Describe the rule in natural language
4. Note key features (colors, shapes, positions, symmetries)
5. Suggest implementation approach (rotation? fill? pattern matching?)

Output format:
PATTERN: [One sentence describing the transformation]
OBSERVATIONS:
- [Key observation 1]
- [Key observation 2]
...
APPROACH: [High-level implementation strategy]
CONFIDENCE: [high/medium/low]
"""
```

**Deliverables:**
- `src/arc_prometheus/cognitive_cells/analyst.py`
- `tests/test_analyst.py` (15+ tests)
- Integration with Programmer (analyst output → programmer input)

**Success Criteria:**
- [ ] Analyst generates natural language specifications
- [ ] Specifications capture key transformation patterns
- [ ] 15+ tests passing

---

### **Task 3.2: Enhanced Programmer** (Day 1, 2-3 hours)

**Purpose:** Accept Analyst specifications as input (currently takes raw task)

**Changes:**
```python
# Before: Programmer.generate_solver(task_json)
# After: Programmer.generate_solver(task_json, analyst_spec=None)

def generate_solver(self, task_json, analyst_spec=None):
    """
    Generate solver code.

    If analyst_spec provided: Use it as guidance (AI Civilization mode)
    If None: Direct generation from examples (Phase 2 mode)
    """
    if analyst_spec:
        prompt = self._create_guided_prompt(task_json, analyst_spec)
    else:
        prompt = self._create_direct_prompt(task_json)

    return self.llm.generate(prompt)
```

**Deliverables:**
- Updated `cognitive_cells/programmer.py`
- Tests for guided generation mode

---

### **Task 3.3: Tagger Agent** (Day 2, 4-5 hours)

**Purpose:** Classify successful solvers by technique (needed for Crossover)

**What Tagger Does:**
1. Analyzes solver code
2. Identifies techniques used (rotation, fill, symmetry, pattern_matching, etc.)
3. Tags stored in solver library
4. **Key insight:** Enables technique-based crossover

**Implementation:**

```python
# src/arc_prometheus/cognitive_cells/tagger.py

class Tagger:
    """Classifies solver techniques for crossover selection."""

    TECHNIQUE_TAXONOMY = [
        "rotation", "flip", "transpose",
        "color_fill", "pattern_copy", "symmetry",
        "grid_partition", "object_detection",
        "counting", "conditional_logic",
        "array_manipulation", "neighborhood_analysis"
    ]

    def tag_solver(self, solver_code: str, task_json: dict) -> List[str]:
        """
        Analyze solver code and identify techniques used.

        Returns: List of technique tags
        """
        # 1. Static code analysis
        static_tags = self._static_analysis(solver_code)

        # 2. LLM-based semantic analysis
        llm_tags = self._llm_analysis(solver_code, task_json)

        # 3. Combine and validate
        tags = list(set(static_tags + llm_tags))

        return tags

    def _static_analysis(self, code: str) -> List[str]:
        """Pattern matching for obvious techniques."""
        tags = []

        if "np.rot90" in code or "rotate" in code:
            tags.append("rotation")
        if "np.flip" in code or "flip" in code:
            tags.append("flip")
        if "np.transpose" in code:
            tags.append("transpose")
        # ... more patterns

        return tags

    def _llm_analysis(self, code: str, task_json: dict) -> List[str]:
        """LLM identifies semantic techniques."""
        prompt = f"""Analyze this ARC solver code and identify techniques used.

Code:
{code}

Available techniques: {', '.join(self.TECHNIQUE_TAXONOMY)}

Which techniques does this solver use? Return as comma-separated list.
"""
        response = self.llm.generate(prompt, temperature=0.2)
        return self._parse_tags(response)
```

**Deliverables:**
- `src/arc_prometheus/cognitive_cells/tagger.py`
- `tests/test_tagger.py` (10+ tests)
- Technique taxonomy definition

**Success Criteria:**
- [ ] Tagger identifies 3-5 techniques per successful solver
- [ ] Static + LLM analysis combined
- [ ] 10+ tests passing

---

### **Task 3.4: Crossover Agent** (Day 2-3, 8-10 hours) ⭐ CRITICAL

**Purpose:** The unique differentiator - fuse techniques from different solvers

**What Crossover Does:**
1. Selects 2 parent solvers with complementary techniques
2. LLM prompt: "Fuse these capabilities into a more general solver"
3. Tests offspring against both parent tasks
4. **Key insight:** Creates solutions that didn't exist in training data

**Implementation:**

```python
# src/arc_prometheus/cognitive_cells/crossover.py

class CrossoverAgent:
    """Fuses techniques from multiple solvers to create novel solutions."""

    def crossover(
        self,
        parent1: Solver,
        parent2: Solver,
        target_task: dict
    ) -> Solver:
        """
        Create offspring solver by fusing parent techniques.

        Args:
            parent1: First solver (e.g., good at rotation)
            parent2: Second solver (e.g., good at color fill)
            target_task: New task to solve

        Returns:
            Offspring solver combining both techniques
        """
        # 1. Get parent analyses
        parent1_tags = parent1.tags  # e.g., ["rotation", "symmetry"]
        parent2_tags = parent2.tags  # e.g., ["color_fill", "pattern_copy"]

        # 2. Create fusion prompt
        prompt = self._create_fusion_prompt(
            parent1.code, parent1_tags,
            parent2.code, parent2_tags,
            target_task
        )

        # 3. Generate offspring code
        offspring_code = self.llm.generate(prompt, temperature=0.6)

        # 4. Validate and return
        offspring = Solver(
            code=offspring_code,
            parent_ids=[parent1.id, parent2.id],
            generation=max(parent1.generation, parent2.generation) + 1
        )

        return offspring

    def _create_fusion_prompt(self, code1, tags1, code2, tags2, task):
        return f"""Create a new ARC solver by fusing techniques from two successful solvers.

Parent Solver 1 (techniques: {', '.join(tags1)}):
{code1}

Parent Solver 2 (techniques: {', '.join(tags2)}):
{code2}

New Task to Solve:
{format_task(task)}

Your task:
1. Identify the key technique from each parent
2. Fuse them into a MORE GENERAL solver
3. The offspring should handle cases that neither parent could solve alone
4. Use numpy for array operations

Generate ONLY the fused solve() function:
"""

    def select_parents(
        self,
        solver_library: List[Solver],
        target_task: dict,
        selection_strategy: str = "complementary"
    ) -> Tuple[Solver, Solver]:
        """
        Select 2 parents for crossover based on technique diversity.

        Strategies:
        - complementary: Max technique difference (rotation + fill)
        - similarity: Similar techniques but different implementations
        - fitness: Top 2 highest fitness solvers
        """
        if selection_strategy == "complementary":
            # Find solvers with minimal tag overlap
            best_pair = None
            max_diversity = 0

            for s1, s2 in combinations(solver_library, 2):
                overlap = len(set(s1.tags) & set(s2.tags))
                total = len(set(s1.tags) | set(s2.tags))
                diversity = 1 - (overlap / total) if total > 0 else 0

                if diversity > max_diversity:
                    max_diversity = diversity
                    best_pair = (s1, s2)

            return best_pair

        # ... other strategies
```

**Deliverables:**
- `src/arc_prometheus/cognitive_cells/crossover.py`
- `tests/test_crossover.py` (12+ tests)
- Parent selection strategies
- Fusion prompt templates

**Success Criteria:**
- [ ] Crossover generates novel solvers combining parent techniques
- [ ] Offspring tested on both parent tasks
- [ ] Multiple selection strategies implemented
- [ ] 12+ tests passing

---

### **Task 3.5: Population-Based Evolution** (Day 3, 6-8 hours)

**Purpose:** Move from single-lineage to true genetic algorithm

**What Population Evolution Does:**
1. Maintains population of diverse solvers
2. Selection: Tournament or fitness-based
3. Breeding: Crossover between high-fitness parents
4. Mutation: Refiner improves offspring
5. **Key insight:** Parallel exploration of solution space

**Implementation:**

```python
# src/arc_prometheus/evolutionary_engine/population_evolution.py

class PopulationEvolution:
    """Genetic algorithm with multi-agent cognitive cells."""

    def __init__(
        self,
        population_size: int = 20,
        selection_pressure: float = 0.3,
        mutation_rate: float = 0.2,
        crossover_rate: float = 0.5
    ):
        self.population_size = population_size
        self.selection_pressure = selection_pressure
        self.mutation_rate = mutation_rate
        self.crossover_rate = crossover_rate

        # Cognitive cells
        self.analyst = Analyst()
        self.programmer = Programmer()
        self.refiner = Refiner()
        self.tagger = Tagger()
        self.crossover = CrossoverAgent()

    def evolve_population(
        self,
        task_json: dict,
        max_generations: int = 10,
        target_fitness: float = 13.0
    ) -> List[Solver]:
        """
        Evolve population of solvers for a single ARC task.

        Returns: Final population sorted by fitness
        """
        # 1. Analyst phase: Understand the task
        analysis = self.analyst.analyze_task(task_json)

        # 2. Initialize population
        population = self._initialize_population(task_json, analysis)

        # 3. Evaluate initial fitness
        population = self._evaluate_population(population, task_json)

        # 4. Evolution loop
        for gen in range(max_generations):
            # 4a. Selection
            parents = self._select_parents(population)

            # 4b. Breeding (crossover + mutation)
            offspring = []

            for i in range(0, len(parents), 2):
                if random.random() < self.crossover_rate:
                    # Crossover
                    child = self.crossover.crossover(
                        parents[i], parents[i+1], task_json
                    )
                else:
                    # Direct copy
                    child = copy.deepcopy(parents[i])

                # Mutation (Refiner)
                if random.random() < self.mutation_rate:
                    child = self._mutate(child, task_json)

                offspring.append(child)

            # 4c. Tag offspring
            for child in offspring:
                child.tags = self.tagger.tag_solver(child.code, task_json)

            # 4d. Evaluate offspring
            offspring = self._evaluate_population(offspring, task_json)

            # 4e. Combine and select survivors
            combined = population + offspring
            population = self._select_survivors(combined, self.population_size)

            # 4f. Check termination
            best_fitness = max(s.fitness for s in population)
            if best_fitness >= target_fitness:
                break

        # 5. Return final population
        return sorted(population, key=lambda s: s.fitness, reverse=True)

    def _initialize_population(self, task_json, analysis):
        """Generate initial population using Analyst + Programmer."""
        population = []

        for i in range(self.population_size):
            # Generate with variety
            code = self.programmer.generate_solver(
                task_json,
                analyst_spec=analysis,
                temperature=0.3 + (i * 0.02)  # Increasing diversity
            )

            solver = Solver(
                code=code,
                generation=0,
                id=f"gen0_{i}"
            )

            # Tag solver
            solver.tags = self.tagger.tag_solver(code, task_json)

            population.append(solver)

        return population

    def _select_parents(self, population):
        """Tournament selection."""
        parents = []
        k = 3  # Tournament size

        for _ in range(len(population)):
            tournament = random.sample(population, k)
            winner = max(tournament, key=lambda s: s.fitness)
            parents.append(winner)

        return parents

    def _mutate(self, solver, task_json):
        """Mutation via Refiner agent."""
        # Find a failing test case (if any)
        failures = self._get_failures(solver, task_json)

        if failures:
            # Refine based on failure
            improved_code = self.refiner.refine_solver(
                task_json,
                solver.code,
                failures[0]
            )
        else:
            # No failures - try general improvement
            improved_code = solver.code

        return Solver(
            code=improved_code,
            generation=solver.generation + 1,
            parent_ids=[solver.id]
        )
```

**Deliverables:**
- `src/arc_prometheus/evolutionary_engine/population_evolution.py`
- `tests/test_population_evolution.py` (15+ tests)
- Solver library structure (SQLite or JSON)
- Population statistics tracking

**Success Criteria:**
- [ ] Population maintains diversity across generations
- [ ] Crossover creates novel solvers
- [ ] Best fitness improves over generations
- [ ] 15+ tests passing

---

### **Task 3.6: Kaggle Baseline Submission** (Day 3, 4-6 hours)

**Purpose:** Submit pure AI Civilization to competition

**Kaggle Notebook Structure:**

```python
# Cell 1: Load all agents
from arc_prometheus.cognitive_cells import Analyst, Programmer, Refiner, Tagger
from arc_prometheus.cognitive_cells.crossover import CrossoverAgent
from arc_prometheus.evolutionary_engine import PopulationEvolution

# Cell 2: Configure population evolution
pop_evo = PopulationEvolution(
    population_size=10,  # Smaller for time constraints
    max_generations=5,
    crossover_rate=0.5,
    mutation_rate=0.2
)

# Cell 3: Load tasks
with open('/kaggle/input/arc-agi_evaluation_challenges_merged.json') as f:
    tasks = json.load(f)

# Cell 4: Evolve populations
submission = {}

for task_id, task in tasks.items():
    # 1. Evolve population of solvers
    population = pop_evo.evolve_population(task, max_generations=5)

    # 2. Select best 2 solvers (pass@2)
    best_solver = population[0]
    second_best = population[1] if len(population) > 1 else population[0]

    # 3. Apply to test inputs
    predictions = []
    for test_input in task["test"]:
        attempt_1 = execute_solver(best_solver.code, test_input["input"])
        attempt_2 = execute_solver(second_best.code, test_input["input"])

        predictions.append({
            "attempt_1": attempt_1.tolist(),
            "attempt_2": attempt_2.tolist()
        })

    submission[task_id] = predictions

# Cell 5: Save
with open('submission.json', 'w') as f:
    json.dump(submission, f)
```

**Deliverables:**
- `kaggle_baseline_civilization.ipynb`
- Baseline leaderboard score
- Performance analysis

**Expected Baseline Score:** 15-25% (competitive with pure LLM approaches)

---

## Experimental Variations (After Baseline)

Once baseline is established, run these experiments:

### **Experiment 1: Single Model Active Inference** (4-6 hours)

**Goal:** Measure value-add from active inference alone

**Approach:**
1. Distill population evolution outputs into one model (Gemma 2 27B)
2. Implement active inference (fine-tune on test examples)
3. Submit to Kaggle
4. **Compare:** Civilization baseline vs Active inference

**Expected:** +5-10% improvement (20-35% total)

### **Experiment 2: Multi-Agent Active Inference** (6-8 hours)

**Goal:** Does multi-agent active inference beat single model?

**Approach:**
1. Each agent (Analyst, Programmer, Refiner) does active inference separately
2. Pipeline them together at inference time
3. Submit to Kaggle
4. **Compare:** Single model vs Multi-agent active inference

**Expected:** +2-5% improvement over Experiment 1 (22-40% total)

---

## Timeline

### **Days 1-3: Complete AI Civilization (Baseline)**
- Day 1: Analyst (6-8h) + Enhanced Programmer (2-3h)
- Day 2: Tagger (4-5h) + Crossover (8-10h)
- Day 3: Population Evolution (6-8h) + Kaggle Baseline (4-6h)

**Total: 30-40 hours** (aggressive but achievable over 3 days)

### **Days 4-5: Experimental Variations (Optional)**
- Experiment 1: Single model active inference (4-6h)
- Experiment 2: Multi-agent active inference (6-8h)

**Total: 10-14 hours** (if time permits)

---

## Success Criteria

**Phase 3 Complete (AI Civilization):**
- [ ] All 5 agents implemented (Analyst, Programmer, Refiner, Tagger, Crossover)
- [ ] Population-based evolution working
- [ ] 60+ new tests passing
- [ ] Baseline Kaggle submission: 15-25% score

**Experimental Validation:**
- [ ] Experiment 1 complete: Measure active inference value-add
- [ ] Experiment 2 complete: Measure multi-agent vs single model
- [ ] Analysis report: What provides the most value?

**Research Question Answered:**
> Can AI Civilization (multi-agent evolution with crossover) solve novel problems competitively?

---

## Key Insights from ARC Prize Official Guide

### Active Inference (SOTA 34%)
- Fine-tune on each test task's 3 training examples at runtime
- Artificially expand to 30+ examples via augmentation
- Jack Cole's approach - proven to work

### Hardware (96GB GPU)
- Kaggle provides L4x4 with 96GB memory
- Can use larger models (13B-27B)
- Enables active inference at scale

### François's Recommendation
> "The most promising category of solutions is one that we haven't really seen in practice so far... **augment discrete program search with deep learning driven intuition**."

**Our approach aligns:** Multi-agent LLMs provide intuition, population evolution does the search.

---

## Research Hypothesis

**H1:** Multi-agent AI Civilization with crossover outperforms single-model approaches on novel tasks
**H2:** Active inference provides additional benefit on top of multi-agent architecture
**H3:** Multi-agent active inference > single-model active inference

**Validation:** Run all 3 experiments and compare leaderboard scores

---

## Next Immediate Action

**Start Phase 3, Task 3.1:** Implement Analyst Agent

This is the foundation for the entire civilization - separating pattern understanding from code generation.

**Estimated time:** 6-8 hours
**Expected output:** `src/arc_prometheus/cognitive_cells/analyst.py` with 15+ tests
