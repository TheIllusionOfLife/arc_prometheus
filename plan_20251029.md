# Development Plan - ARC-Prometheus (October 29, 2025)

## Executive Summary

This plan synthesizes the original Phase roadmap with insights from 5 comprehensive external reviews (Ge, Ch, Cl, Co, GeC). All reviewers agreed: **the current implementation is excellent as a research prototype (8.5/10), but Phase 3 requires infrastructure investment** before scaling.

**Current Status:**
- ✅ Phase 1 Complete: End-to-end pipeline (Crucible + Cognitive Cells)
- ✅ Phase 2 Complete: Evolutionary mechanisms (Fitness + Refiner + Evolution Loop)
- ✅ 129 tests passing (100% success rate)
- ✅ Real API validation successful (Gemini 2.5 Flash Lite)

**Key Insight from Reviews:**
The gap between Phase 2 (local, single-process) and Phase 3 (distributed, cloud) is significant. Bridging this gap requires **foundational infrastructure work** before implementing new features.

---

## Critical Themes from Reviews

All reviewers identified these priorities:

1. **Security** (Critical): Docker sandbox required before production
2. **Flexibility** (Important): Config externalization for experiments
3. **Cost Management** (Important): LLM caching and rate limiting
4. **Scalability** (Critical): Database + queue infrastructure for Phase 3
5. **Algorithm Maturity** (Important): Population-based evolution vs single-solver
6. **Observability** (Important): Metrics, logging, visualization

---

## Short-Term Plan (1-2 Weeks)

### Week 1: Quick Wins (No Infrastructure Changes)

**Goal:** Improve experimental flexibility and reduce API costs without major refactoring.

#### Task 1.1: Config Externalization (Ge, GeC suggestion)
**Priority:** High | **Effort:** 1 day | **Risk:** Low

- [ ] Create `src/arc_prometheus/utils/cli_config.py`
- [ ] Add CLI arguments to evolution demo:
  - `--model` (default: gemini-2.5-flash-lite)
  - `--temperature` (default: 0.4)
  - `--timeout-llm` (default: 60)
  - `--timeout-eval` (default: 5)
  - `--target-fitness` (default: None)
  - `--max-generations` (default: 5)
- [ ] Keep `config.py` defaults, CLI overrides
- [ ] Add tests for CLI argument parsing
- [ ] Update README with new CLI usage examples

**Benefit:** Experiment iterations without code changes

#### Task 1.2: LLM Response Caching (GeC, Co suggestion)
**Priority:** High | **Effort:** 1-2 days | **Risk:** Low

- [ ] Create `src/arc_prometheus/utils/llm_cache.py`
  - Hash prompt → cache key
  - Store responses in SQLite (`~/.arc_prometheus/llm_cache.db`)
  - TTL support (default: 7 days)
- [ ] Integrate cache into `programmer.py` and `refiner.py`
- [ ] Add `--no-cache` CLI flag
- [ ] Add cache statistics (hit rate, cost saved)
- [ ] Add tests for cache behavior

**Benefit:**
- Reduce API costs during debugging (10x+ reduction)
- Improve experiment reproducibility
- Faster iteration during development

#### Task 1.3: Error Pattern Classification (Cl suggestion)
**Priority:** Medium | **Effort:** 1 day | **Risk:** Low

- [ ] Create `src/arc_prometheus/evolutionary_engine/error_classifier.py`
- [ ] Implement `ErrorType` enum (SYNTAX, LOGIC, TIMEOUT, MEMORY, TYPE)
- [ ] Add `classify_error(fitness_result) -> ErrorType`
- [ ] Log error statistics in evolution loop
- [ ] Add specialized refiner prompts per error type (Phase 2 enhancement)
- [ ] Add tests for classification logic

**Benefit:** Better understanding of failure modes, smarter refinement

---

### Week 2: Foundation for Phase 3

**Goal:** Establish infrastructure foundations that enable Phase 3 scaling.

#### Task 2.1: Docker Sandbox Implementation (ALL reviewers - CRITICAL)
**Priority:** Critical | **Effort:** 2-3 days | **Risk:** Medium

- [ ] Create `docker/sandbox.Dockerfile`
  - Base: `python:3.13-slim`
  - Install: `numpy==2.0.0` only
  - Non-root user (UID 1000)
- [ ] Create `src/arc_prometheus/crucible/docker_sandbox.py`
  - Use `docker-py` library
  - Enforce: read-only FS, no network, resource limits
  - Timeout at container level (not just Python)
- [ ] Create `ExecutionEnvironment` Protocol
- [ ] Refactor existing code to use Protocol
- [ ] Add CLI flag: `--sandbox-mode [multiprocess|docker]`
- [ ] Add Docker integration tests
- [ ] Update README with Docker setup instructions

**Security Checklist:**
- [ ] Network disabled
- [ ] Filesystem read-only
- [ ] Memory limit: 512MB
- [ ] CPU limit: 50%
- [ ] Process limit: 100
- [ ] Timeout: 5s default

**Benefit:** Production-grade security, safe for cloud deployment

#### Task 2.2: Database Schema Design (Cl suggestion - CRITICAL)
**Priority:** Critical | **Effort:** 1-2 days | **Risk:** Low

- [ ] Install SQLAlchemy + Alembic
- [ ] Create `src/arc_prometheus/storage/models.py`:
  - `Solver` table (id, task_id, generation, code, fitness, parent_id, tags, created_at)
  - `Evolution` table (id, task_id, generation, best_fitness, solver_id, created_at)
  - `Execution` table (id, solver_id, example_id, success, output, error, duration)
- [ ] Create initial Alembic migration
- [ ] Create `src/arc_prometheus/storage/repository.py`
  - `SolverRepository` abstract interface
  - `SQLiteSolverRepository` implementation
- [ ] Add tests for repository operations
- [ ] Add CLI flag: `--db-path` (default: `~/.arc_prometheus/solvers.db`)

**Benefit:**
- Persistent evolution history
- Foundation for Phase 3 features (Tagger, Crossover)
- Research reproducibility

#### Task 2.3: Metrics & Monitoring (Ch, Cl suggestion)
**Priority:** Important | **Effort:** 1 day | **Risk:** Low

- [ ] Create `src/arc_prometheus/utils/metrics.py`
- [ ] Add Prometheus metrics:
  - `solver_generations_total` (Counter)
  - `solver_fitness_score` (Histogram)
  - `llm_api_calls_total` (Counter)
  - `llm_api_cost_usd` (Counter)
  - `execution_duration_seconds` (Histogram)
- [ ] Add metrics endpoint (optional: FastAPI server)
- [ ] Create Grafana dashboard JSON
- [ ] Add cost estimation (Gemini API pricing)
- [ ] Update README with metrics documentation

**Benefit:** Visibility into system behavior, cost tracking

---

## Medium-Term Plan (1-2 Months)

### Month 1: Phase 3.1 - Population-Based Evolution

**Goal:** Transform single-solver evolution into population-based genetic algorithm.

#### Task 3.1: Population Manager (GeC, Ch suggestion)
**Priority:** High | **Effort:** 3-4 days | **Risk:** Medium

- [ ] Create `src/arc_prometheus/evolutionary_engine/population.py`
  - `Population` class (size: 20 solvers)
  - Generation management
  - Fitness tracking per individual
- [ ] Create `src/arc_prometheus/evolutionary_engine/selection.py`
  - Elite selection (top 20%)
  - Roulette wheel selection
  - Tournament selection
- [ ] Refactor `evolution_loop.py` to use `Population`
- [ ] Update tests for population-based evolution
- [ ] Add demo script: `scripts/demo_phase3_1_population.py`

**Key Changes:**
- Generation 0: Generate 20 initial solvers (not 1)
- Each generation: Evaluate all 20, select top performers
- Mutation: Apply probabilistically (30% chance)
- Track diversity metrics (unique code patterns)

**Benefit:** Avoid local optima, explore solution space broadly

#### Task 3.2: Solver Library Implementation
**Priority:** High | **Effort:** 2-3 days | **Risk:** Low

- [ ] Extend `SolverRepository` for queries:
  - Get best solver for task
  - Get solver by generation
  - Get solver ancestry (parent chain)
  - Search by tags (Phase 3.2 preparation)
- [ ] Add solver comparison API
- [ ] Add solver export/import (JSON format)
- [ ] Create CLI tool: `arc-prometheus solver list/show/export`
- [ ] Add comprehensive tests
- [ ] Update README with CLI usage

**Benefit:** Reusable solver database, foundation for Crossover

---

### Month 2: Phase 3.2 - Tagger Agent

**Goal:** Classify successful solvers by technique for intelligent Crossover.

#### Task 3.3: Tagger Agent Implementation
**Priority:** High | **Effort:** 3-4 days | **Risk:** Medium

- [ ] Create `src/arc_prometheus/cognitive_cells/tagger.py`
- [ ] Define technique taxonomy (based on ARC patterns):
  - Spatial: rotation, reflection, translation, scaling
  - Fill: flood_fill, pattern_fill, color_fill
  - Selection: filter, mask, crop
  - Aggregation: count, sum, max, min
  - Topology: connect, separate, boundary
- [ ] Implement `tag_solver(code: str) -> list[str]`
  - LLM-based code analysis
  - Pattern matching for common operations
- [ ] Create prompt template for tagging
- [ ] Store tags in `Solver.tags` (JSON column)
- [ ] Add tests with known code patterns
- [ ] Create demo script: `scripts/demo_phase3_2_tagger.py`

**Benefit:** Enable technique-aware Crossover, analyze solver strategies

#### Task 3.4: Visualization Dashboard (Ch suggestion)
**Priority:** Medium | **Effort:** 3-4 days | **Risk:** Low

- [ ] Choose framework: Streamlit (quick) or FastAPI + React (scalable)
- [ ] Implement pages:
  - Evolution timeline (fitness over generations)
  - Population diversity (unique techniques)
  - Task success heatmap (which tasks solved by which techniques)
  - Cost tracker (API calls, estimated USD)
- [ ] Real-time updates during evolution
- [ ] Export reports (PDF/PNG)
- [ ] Deploy documentation

**Benefit:** Storytelling for research/pitches, monitoring during experiments

---

## Long-Term Plan (3-6 Months)

### Phase 3.3: Crossover Agent (Month 3)

**Goal:** Combine successful solvers with complementary techniques.

#### Task 4.1: Crossover Implementation
**Priority:** High | **Effort:** 4-5 days | **Risk:** High

- [ ] Create `src/arc_prometheus/cognitive_cells/crossover.py`
- [ ] Implement crossover strategies:
  - **Function-level**: Extract functions from 2 solvers, combine
  - **Logic fusion**: LLM combines transformation rules
  - **Ensemble**: Try both solvers, return best result
- [ ] Implement `crossover_solver(solver_a, solver_b, tags) -> str`
- [ ] Add crossover probability to evolution loop (10-20%)
- [ ] Track crossover success rate
- [ ] Add tests with known code pairs
- [ ] Create demo script: `scripts/demo_phase3_3_crossover.py`

**Benefit:** Discover novel solver combinations, accelerate evolution

---

### Phase 3.4: Distributed Processing (Month 4-5)

**Goal:** Scale to process entire ARC dataset (400+ tasks) in parallel.

#### Task 4.2: Celery Task Queue (Cl suggestion)
**Priority:** Critical | **Effort:** 5-7 days | **Risk:** High

- [ ] Install Celery + RabbitMQ + Redis
- [ ] Create `src/arc_prometheus/tasks/worker.py`
  - Task: `generate_solver_task(task_id, train_pairs)`
  - Task: `evaluate_fitness_task(solver_id, task_file, code)`
  - Task: `refine_solver_task(solver_id)`
  - Task: `crossover_task(solver_a_id, solver_b_id)`
- [ ] Implement result backend (Redis)
- [ ] Add retry logic (max 3 retries)
- [ ] Add rate limiting (Gemini API: 15 RPM)
- [ ] Create orchestrator: `run_distributed_evolution.py`
- [ ] Add monitoring (Flower dashboard)
- [ ] Docker Compose setup for local testing
- [ ] Kubernetes manifests for cloud deployment

**Architecture:**
```
Orchestrator (main process)
    ↓
RabbitMQ (task queue)
    ↓
10x Celery Workers (parallel execution)
    ↓
PostgreSQL (results)
```

**Benefit:** Process 400 tasks in hours instead of days

#### Task 4.3: Cloud Deployment (Cl, Ch suggestion)
**Priority:** Important | **Effort:** 3-5 days | **Risk:** High

- [ ] Choose platform: GCP (recommended) or AWS
- [ ] Create Kubernetes manifests:
  - Deployment: arc-prometheus-worker (10 replicas)
  - Service: RabbitMQ, PostgreSQL, Redis
  - ConfigMap: environment variables
  - Secret: GEMINI_API_KEY, DB credentials
- [ ] Setup CI/CD pipeline (GitHub Actions):
  - Build Docker image on merge to main
  - Push to container registry
  - Deploy to Kubernetes cluster
- [ ] Add horizontal autoscaling (HPA)
- [ ] Setup monitoring (Prometheus + Grafana)
- [ ] Add alerting (Slack/Email)
- [ ] Create runbook for operations

**Benefit:** Production-ready infrastructure, continuous deployment

---

### Phase 3.5: Optimization & Evaluation (Month 6)

**Goal:** Optimize system performance and validate against ARC Prize benchmarks.

#### Task 5.1: Hyperparameter Tuning
**Priority:** Medium | **Effort:** 3-4 days | **Risk:** Medium

- [ ] Install Optuna
- [ ] Define search space:
  - LLM temperature (0.2 - 1.0)
  - Population size (10 - 50)
  - Mutation probability (0.1 - 0.5)
  - Crossover probability (0.05 - 0.3)
  - Max generations (3 - 20)
- [ ] Implement objective function (maximize test accuracy)
- [ ] Run 100 trials on validation set
- [ ] Analyze results, update defaults
- [ ] Document findings

**Benefit:** Maximize solver performance, reduce wasted compute

#### Task 5.2: ARC Prize Evaluation
**Priority:** High | **Effort:** 2-3 days | **Risk:** Low

- [ ] Download official evaluation set (100 tasks)
- [ ] Run full evolution pipeline on all tasks
- [ ] Calculate official metrics:
  - Pass@1 accuracy (best solver per task)
  - Pass@k accuracy (top-k solvers per task)
  - Generalization rate (test/train ratio)
- [ ] Generate leaderboard comparison
- [ ] Create submission package (if competing)
- [ ] Write technical report

**Benefit:** Objective validation, external credibility

#### Task 5.3: Multi-LLM Support (Ch suggestion)
**Priority:** Medium | **Effort:** 2-3 days | **Risk:** Low

- [ ] Create `src/arc_prometheus/utils/llm_provider.py`
- [ ] Define `LLMProvider` Protocol
- [ ] Implement providers:
  - `GeminiProvider` (existing)
  - `OpenAIProvider` (GPT-4)
  - `AnthropicProvider` (Claude)
  - `LocalLLMProvider` (Ollama/vLLM)
- [ ] Add CLI flag: `--llm-provider`
- [ ] Add cost comparison tracking
- [ ] Update documentation

**Benefit:** Cost optimization, vendor independence, flexibility

---

## Risk Management

### High-Risk Items

| Risk | Mitigation |
|------|-----------|
| Docker complexity breaks existing tests | Maintain both multiprocess and Docker modes, gradual migration |
| LLM API rate limits hit during scaling | Implement exponential backoff, queue management, local LLM fallback |
| Database schema changes break migrations | Use Alembic properly, test migrations on staging |
| Celery worker failures cause data loss | Idempotent tasks, result persistence, retry logic |
| Cloud costs exceed budget | Start with small cluster, monitor costs, autoscaling limits |
| Population evolution degrades quality | Track fitness over time, implement diversity metrics, rollback if needed |

### Mitigation Strategies

1. **Incremental deployment**: Feature flags for new components
2. **Backward compatibility**: Keep existing code paths working
3. **Comprehensive testing**: Integration tests before production
4. **Monitoring**: Metrics and alerts at every layer
5. **Documentation**: Runbooks for common failure scenarios

---

## Success Metrics

### Short-Term (Week 1-2)
- [ ] LLM cache hit rate > 70% during development
- [ ] Docker sandbox passes all existing tests
- [ ] Config externalization reduces experiment setup time by 50%
- [ ] Zero security incidents with Docker sandbox

### Medium-Term (Month 1-2)
- [ ] Population evolution improves best fitness by 20% vs single-solver
- [ ] Solver library contains 500+ unique solvers
- [ ] Tagger achieves 85%+ accuracy on known patterns
- [ ] Dashboard provides real-time evolution visualization

### Long-Term (Month 3-6)
- [ ] Distributed system processes 400 tasks in < 24 hours
- [ ] ARC Prize evaluation shows > 10% task success rate
- [ ] Crossover generates novel solvers not achievable by mutation alone
- [ ] System runs reliably on cloud for 1 month without intervention

---

## Resource Requirements

### Development Time
- **Short-term (Weeks 1-2):** 40-60 hours (1.5 person-weeks)
- **Medium-term (Months 1-2):** 120-160 hours (4-5 person-weeks)
- **Long-term (Months 3-6):** 200-250 hours (8-10 person-weeks)

**Total:** ~20 person-weeks (5 months for solo developer)

### Infrastructure Costs (Estimated)

**Development Phase:**
- LLM API (Gemini): $50-100/month
- Local Docker: $0
- Local SQLite: $0
- **Total:** $50-100/month

**Production Phase (Cloud):**
- Kubernetes cluster (GCP): $200-300/month
- PostgreSQL: $50-100/month
- LLM API (scaled): $200-500/month
- Monitoring/logging: $50/month
- **Total:** $500-950/month

### External Dependencies
- Gemini API key (existing)
- Docker installed
- Kubernetes cluster (Month 4+)
- PostgreSQL (Month 2+, can use SQLite initially)
- RabbitMQ/Redis (Month 4+)

---

## Alignment with Reviews

### How This Plan Addresses Each Reviewer

**Ge (実用的・シンプル):**
- ✅ Task 1.1: Config externalization (Week 1)
- ✅ Task 2.2: SolverRepository abstraction (Week 2)
- ✅ Dependency management: Add poetry in Month 4

**Ch (包括的・戦略的):**
- ✅ Task 3.1: Population-based evolution (Month 1)
- ✅ Task 3.4: Visualization dashboard (Month 2)
- ✅ Task 5.2: ARC Prize objective metrics (Month 6)
- ✅ Python 3.10-3.12 support: Document in README (Week 1)

**Cl (技術的・詳細):**
- ✅ Task 2.1: Docker sandbox (Week 2) - used exact code examples
- ✅ Task 2.2: Database schema (Week 2) - SQLAlchemy models
- ✅ Task 4.2: Celery queue (Month 4) - Kubernetes manifests
- ✅ Task 2.3: Prometheus metrics (Week 2)
- ✅ All Critical/Important priorities addressed

**Co (簡潔・要点明確):**
- ✅ Docker sandbox (Week 2)
- ✅ LLM cache + cost metrics (Week 1)
- ✅ Database persistence (Week 2)
- ✅ README badges (Week 1)

**GeC (構造化・バランス):**
- ✅ Proposal 1: Docker security (Week 2)
- ✅ Proposal 2: Population algorithm (Month 1)
- ✅ Proposal 3: LLM caching (Week 1)
- ✅ Proposal 4: Config externalization (Week 1)

---

## Next Immediate Actions (This Week)

### Day 1-2: Config & CLI
1. Create `cli_config.py` with argument parsing
2. Update demo scripts to accept CLI args
3. Add tests and README documentation

### Day 3-4: LLM Caching
1. Implement `llm_cache.py` with SQLite backend
2. Integrate into `programmer.py` and `refiner.py`
3. Add cache statistics and `--no-cache` flag

### Day 5: Foundation Prep
1. Research Docker sandbox best practices
2. Design `ExecutionEnvironment` Protocol
3. Plan database schema with SQLAlchemy
4. Create Week 2 implementation tickets

---

## Conclusion

This plan transforms ARC-Prometheus from a **research prototype** into a **production-ready AGI research platform** over 6 months. The approach is:

1. **Pragmatic**: Quick wins first (config, caching) before major refactoring
2. **Safe**: Security (Docker) and persistence (DB) before scaling
3. **Incremental**: Each phase builds on previous infrastructure
4. **Validated**: Success metrics and ARC Prize evaluation

**The critical path is:**
```
Week 1-2 (Foundation) → Month 1-2 (Population + Tools) → Month 3-6 (Distributed + Cloud)
```

**If time/budget is limited, prioritize:**
1. Docker sandbox (security)
2. LLM caching (cost)
3. Population evolution (research value)
4. Database persistence (reproducibility)

Everything else can be deferred without blocking core research goals.

---

**Document Version:** 1.0
**Last Updated:** October 29, 2025
**Next Review:** November 12, 2025 (after Week 2 completion)
