# ARC-Prometheus Architecture Pivot Plan
**Date**: November 2, 2025
**Status**: Planning ‚Üí Implementation

---

## Executive Summary

**Pivot**: From evolution-based solver improvement to test-time multi-persona ensemble
**Reason**: ARC-AGI requires generalization to unseen tasks, not task-specific optimization
**Impact**: Fundamental architecture change, 3 API calls per task, focus on diverse interpretations

---

## The Problem That Led to This Pivot

### Original Architecture Issues

1. **Evolution was task-specific**:
   ```
   Training: Evolve solve() for task A ‚Üí fitness improves for task A
   Kaggle: Task B appears ‚Üí our task-A solver fails
   ```

2. **Refiner agent broken**:
   - Despite improved prompts, LLM rewrites code from scratch
   - No incremental debugging behavior
   - Fitness stays flat across generations (0.0 ‚Üí 0.0 ‚Üí 0.0)

3. **Misaligned with ARC philosophy**:
   - ARC tests: "Can you infer rules from 3 examples and apply to unseen input?"
   - Our system: "Can you evolve task-specific code over 10 generations?"
   - **These are fundamentally different problems**

### Key Insight from User

> "We may need to train our system so that the system can learn from train sets of the private test set on the fly, and come up with the answer."

This means:
- ‚úÖ Kaggle shows us task["train"] with inputs + outputs
- ‚úÖ We analyze those examples in real-time
- ‚úÖ We generate solve() on-the-fly for that specific task
- ‚úÖ We apply solve() to task["test"]["input"]

**What we evolve**: The meta-capability (prompts/agents) to analyze-and-solve NEW tasks, not specific solvers.

---

## New Architecture: Test-Time Multi-Persona Ensemble

### Core Philosophy

**Diversity = Generalization**

Instead of one Analyst interpretation ‚Üí one Programmer implementation, we:
1. Generate 5 diverse interpretations (different expert perspectives)
2. Implement all 5 interpretations as solve() functions
3. Test all 5 on task["train"] (known outputs)
4. Synthesize a 6th diverse solution learning from the 5 attempts
5. Submit best solution + synthesis solution (pass@2 format)

### Flow Diagram

```
Task arrives (train examples + test input)
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ API Call 1: Multi-Persona Analyst          ‚îÇ
‚îÇ Temperature: 1.0 (high diversity)           ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ Input: task["train"] examples              ‚îÇ
‚îÇ Output: 5 AnalysisResult objects            ‚îÇ
‚îÇ   - Geometric Expert                        ‚îÇ
‚îÇ   - Color Expert                            ‚îÇ
‚îÇ   - Object Expert                           ‚îÇ
‚îÇ   - Grid Expert                             ‚îÇ
‚îÇ   - Logic Expert                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ API Call 2: Multi-Solution Programmer      ‚îÇ
‚îÇ Temperature: 0.7 (moderate diversity)       ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ Input: 5 interpretations                    ‚îÇ
‚îÇ Output: 5 solve() functions                 ‚îÇ
‚îÇ   solve_1() ... solve_5()                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Local Validation (No API)                  ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ For each solve_i():                         ‚îÇ
‚îÇ   accuracy_i = test_on_train(solve_i)       ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ Results: [1.0, 0.67, 0.0, 1.0, 0.33]       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ API Call 3: Synthesis Agent                ‚îÇ
‚îÇ Temperature: 0.5 (balanced)                 ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ Input: 5 solutions + accuracies             ‚îÇ
‚îÇ Task: Create 6th DIVERSE solution           ‚îÇ
‚îÇ   - Learn from successful patterns          ‚îÇ
‚îÇ   - Avoid failed approaches                 ‚îÇ
‚îÇ   - Use different algorithm than all 5      ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ Output: solve_synthesis()                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ Final Selection                             ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ best_idx = argmax(accuracies)               ‚îÇ
‚îÇ prediction_1 = solve_best(test_input)       ‚îÇ
‚îÇ prediction_2 = solve_synthesis(test_input)  ‚îÇ
‚îÇ                                             ‚îÇ
‚îÇ Submit: (prediction_1, prediction_2)        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Cost Analysis

**Per task**: 3 API calls (vs original 6+)
- 1 Multi-Persona Analyst
- 1 Multi-Solution Programmer (generates 5 functions)
- 1 Synthesis Agent

**For 100 evaluation tasks**: 300 API calls total

---

## What We Evolve: Multi-Persona Prompt

### Evolution Target

The **persona definitions** in the Multi-Persona Analyst prompt.

**Genetic Representation**:
```python
PersonaGenome = {
    "persona_1": {
        "name": "Geometric Transformation Specialist",
        "focus": "Rotations (90¬∞/180¬∞/270¬∞), reflections (H/V), transpose, symmetry operations",
        "key_question": "Is this a spatial transformation of the grid itself?",
        "emoji": "üî∑"
    },
    "persona_2": {
        "name": "Color Pattern Specialist",
        "focus": "Color changes, filling, replacement rules, color-based conditional logic",
        "key_question": "What color operations transform input to output?",
        "emoji": "üé®"
    },
    # ... personas 3-5
}
```

### Mutation Operators

1. **Swap personas**: Exchange positions (e.g., persona_3 ‚Üî persona_4)
2. **Refine focus**: Add/remove specific operations from focus area
3. **Reword question**: Change the guiding question phrasing for clarity
4. **Replace persona**: Introduce entirely new perspective (e.g., "Fractal Pattern Expert", "Topological Invariant Specialist")

### Crossover Operators

- Combine persona sets from 2 high-performing prompts
- Example: Take best 3 personas from prompt A + best 2 from prompt B

### Fitness Evaluation

**Metric**: Cross-task pass@2 accuracy on held-out validation set

```python
fitness = mean([
    pass_at_2_accuracy(task_1, persona_set),
    pass_at_2_accuracy(task_2, persona_set),
    ...
    pass_at_2_accuracy(task_20, persona_set)
])
```

Where `pass_at_2_accuracy` = 1 if either prediction matches ground truth, else 0.

### Evolution Loop (Optional, Time Permitting)

```python
# Generation 0: Initialize
population = [
    default_personas,
    mutate(default_personas),
    mutate(default_personas),
    mutate(default_personas),
    mutate(default_personas)
]

# Evaluate on validation set (20 held-out tasks)
for persona_set in population:
    fitness[persona_set] = evaluate_on_validation(persona_set, validation_tasks)

# Select top performers
best_k = top_k(population, k=2, by=fitness)

# Next generation
next_gen = [
    best_k[0],  # Elite
    best_k[1],  # Elite
    crossover(best_k[0], best_k[1]),
    mutate(best_k[0]),
    mutate(best_k[1])
]

# Repeat for N generations or until convergence
```

---

## CRITICAL IMPLEMENTATION DETAIL: Structured Output

### Why Structured Output is Essential

The new architecture requires **Gemini Structured Output** for reliability and efficiency:

1. **Type Safety**: JSON schema enforces exact field types and constraints
2. **Token Efficiency**: ~70% reduction vs free-form text parsing
3. **Reliability**: Guaranteed valid JSON (no parsing errors)
4. **Conciseness**: Forces LLM to generate shorter, focused responses
5. **Consistency**: Same format every time (no variation in keywords/structure)

### JSON Schemas for All 3 Agents

**Multi-Persona Analyst Schema**:
```python
MULTI_PERSONA_SCHEMA = {
    "type": "object",
    "properties": {
        "interpretations": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "persona": {"type": "string", "maxLength": 50},
                    "pattern": {"type": "string", "maxLength": 150},
                    "observations": {
                        "type": "array",
                        "items": {"type": "string", "maxLength": 80},
                        "maxItems": 3
                    },
                    "approach": {"type": "string", "maxLength": 100},
                    "confidence": {"type": "string", "enum": ["high", "medium", "low"]}
                },
                "required": ["persona", "pattern", "observations", "approach", "confidence"]
            },
            "minItems": 5,
            "maxItems": 5
        }
    },
    "required": ["interpretations"]
}
```

**Multi-Solution Programmer Schema**:
```python
MULTI_SOLUTION_SCHEMA = {
    "type": "object",
    "properties": {
        "solutions": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "interpretation_id": {"type": "integer", "minimum": 1, "maximum": 5},
                    "code": {"type": "string"},
                    "approach_summary": {"type": "string", "maxLength": 100}
                },
                "required": ["interpretation_id", "code", "approach_summary"]
            },
            "minItems": 5,
            "maxItems": 5
        }
    },
    "required": ["solutions"]
}
```

**Synthesis Agent Schema**:
```python
SYNTHESIS_SCHEMA = {
    "type": "object",
    "properties": {
        "analysis": {
            "type": "object",
            "properties": {
                "successful_patterns": {
                    "type": "array",
                    "items": {"type": "string", "maxLength": 80},
                    "maxItems": 3
                },
                "failed_patterns": {
                    "type": "array",
                    "items": {"type": "string", "maxLength": 80},
                    "maxItems": 3
                },
                "synthesis_strategy": {"type": "string", "maxLength": 150}
            },
            "required": ["successful_patterns", "failed_patterns", "synthesis_strategy"]
        },
        "code": {"type": "string"},
        "diversity_justification": {"type": "string", "maxLength": 100}
    },
    "required": ["analysis", "code", "diversity_justification"]
}
```

### Implementation Pattern

**File**: `src/arc_prometheus/utils/schemas.py`

```python
"""JSON schemas for structured LLM outputs."""

# Export all schemas for use in cognitive cells
__all__ = [
    "MULTI_PERSONA_SCHEMA",
    "MULTI_SOLUTION_SCHEMA",
    "SYNTHESIS_SCHEMA"
]

# Schema definitions as above...
```

**Usage in Cognitive Cells**:

```python
import google.generativeai as genai
from google.generativeai.types import GenerationConfig
from arc_prometheus.utils.schemas import MULTI_PERSONA_SCHEMA

class MultiPersonaAnalyst:
    def analyze_task(self, task: dict) -> list[dict]:
        prompt = self._create_prompt(task)

        generation_config = GenerationConfig(
            temperature=1.0,
            response_mime_type="application/json",
            response_schema=MULTI_PERSONA_SCHEMA
        )

        model = genai.GenerativeModel(self.model_name)
        response = model.generate_content(prompt, generation_config=generation_config)

        # Response is guaranteed valid JSON matching schema
        import json
        result = json.loads(response.text)
        return result["interpretations"]  # List of 5 interpretation dicts
```

### Benefits

1. **Token Reduction**: Prompts become shorter (no need for format examples)
   - Before: ~2000 tokens (examples + format instructions)
   - After: ~600 tokens (just task data + concise instructions)
   - **Savings**: ~70% reduction per API call

2. **Reliability**: No parsing errors from malformed output
   - Before: regex parsing with fallbacks, error handling
   - After: JSON schema guarantees valid structure

3. **Conciseness**: `maxLength` constraints force LLM brevity
   - Pattern: 150 chars max (vs 500+ chars in free-form)
   - Observations: 80 chars each (vs 200+ chars)
   - Total: Fits 5 interpretations in ~1000 tokens (vs 3000+ tokens)

4. **Type Safety**: Enum constraints (e.g., confidence: high/medium/low)
   - No typos ("High" vs "high"), no variations ("confident" vs "high")

5. **Consistency**: Same format every time
   - Simplifies downstream processing (no conditional parsing)
   - Enables reliable caching (exact format matching)

### Prompt Adjustments for Conciseness

**Key changes to all prompts**:

1. **Remove format examples** (schema enforces format)
2. **Add conciseness instructions**:
   ```
   IMPORTANT: Be extremely concise. Each observation must be ‚â§80 chars.
   Pattern description must be ‚â§150 chars. Focus on key insights only.
   ```

3. **Use bullet-point style** in prompts (vs verbose paragraphs)
4. **Reference schema fields by name**:
   ```
   Output JSON with 5 items in "interpretations" array.
   Each item has: persona, pattern, observations (max 3), approach, confidence.
   ```

**Result**: Input prompts shrink from 3000+ tokens to ~1000 tokens, while output becomes more reliable and consistent.

---

## Implementation Tasks

### Phase 1: Core Test-Time Ensemble (PRIORITY)

**Estimated Time**: 3-4 hours

#### Task 1.1: Multi-Persona Analyst
- **File**: `src/arc_prometheus/cognitive_cells/multi_persona_analyst.py`
- **Input**: ARC task dict
- **Output**: List of 5 `AnalysisResult` objects
- **Key**: Prompt with 5 expert personas, temp=1.0 for diversity
- **Parser**: Extract 5 interpretations from single LLM response

#### Task 1.2: Multi-Solution Programmer
- **File**: `src/arc_prometheus/cognitive_cells/multi_solution_programmer.py`
- **Input**: Task + 5 interpretations
- **Output**: List of 5 callable `solve_i()` functions
- **Key**: Prompt to generate 5 solutions in one response, temp=0.7
- **Parser**: Extract and compile 5 separate Python functions

#### Task 1.3: Synthesis Agent
- **File**: `src/arc_prometheus/cognitive_cells/synthesis_agent.py`
- **Input**: Task + 5 solutions + 5 accuracies + 5 interpretations
- **Output**: 1 callable `solve_synthesis()` function
- **Key**: Meta-learning prompt that creates diverse 6th solution, temp=0.5
- **Constraint**: Must use different approach than all 5 previous attempts

#### Task 1.4: Test-Time Ensemble Pipeline
- **File**: `src/arc_prometheus/inference/test_time_ensemble.py`
- **Function**: `solve_task_ensemble(task: dict) -> tuple[np.ndarray, np.ndarray]`
- **Flow**:
  1. Multi-persona analysis (1 API call)
  2. Multi-solution generation (1 API call)
  3. Local validation on task["train"]
  4. Synthesis (1 API call)
  5. Return (best_pred, synthesis_pred)

#### Task 1.5: Validation Script
- **File**: `scripts/validate_ensemble.py`
- **Purpose**: Test ensemble on 10-20 held-out tasks
- **Metrics**:
  - Pass@2 accuracy (either prediction correct)
  - Pass@1 accuracy (best prediction correct)
  - Per-persona success rates
  - Synthesis success rate

**Deliverable**: Working test-time ensemble with measured baseline accuracy

---

### Phase 2: Persona Prompt Evolution (IF TIME ALLOWS)

**Estimated Time**: 2-3 hours

#### Task 2.1: Persona Genome Structure
- **File**: `src/arc_prometheus/evolution/persona_genome.py`
- **Classes**: `PersonaGenome`, `PersonaMutation`, `PersonaCrossover`
- **Mutations**: swap, refine_focus, reword_question, replace_persona
- **Crossover**: Combine top personas from 2 parents

#### Task 2.2: Prompt Evolution Loop
- **File**: `src/arc_prometheus/evolution/prompt_evolution.py`
- **Function**: `evolve_personas(validation_tasks, generations=5)`
- **Evaluation**: Cross-task pass@2 accuracy
- **Selection**: Top-2 elite, breed remaining

#### Task 2.3: Evolution Experiment
- **Script**: `scripts/evolve_personas.py`
- **Config**:
  - Population: 5 persona sets
  - Generations: 3-5
  - Validation: 20 held-out tasks
  - Metric: Pass@2 accuracy
- **Output**: Best persona set + performance curve

**Deliverable**: Evolved persona prompt (if improves over default)

---

### Phase 3: Kaggle Submission Preparation (FINAL)

**Estimated Time**: 1-2 hours

#### Task 3.1: Submission Formatter
- **File**: `src/arc_prometheus/inference/kaggle_submission.py`
- **Input**: List of task dicts (private test set)
- **Output**: `submission.json` in Kaggle pass@2 format
- **Format**:
  ```json
  {
    "task_id_1": [
      {"attempt_1": [[...]], "attempt_2": [[...]]}
    ],
    "task_id_2": [...]
  }
  ```

#### Task 3.2: Offline Inference Pipeline
- **Script**: `scripts/generate_submission.py`
- **Usage**:
  ```bash
  python scripts/generate_submission.py \
    --test-data data/arc-prize-2025/private_test.json \
    --output submission.json \
    --persona-set best_evolved.json
  ```
- **Features**:
  - Progress bar (tqdm)
  - Checkpointing (resume from failures)
  - Error handling (skip broken tasks)

#### Task 3.3: Validation Against Sample Submission
- Compare output format with `sample_submission.json`
- Verify all required fields present
- Check data types (lists, not numpy arrays)

**Deliverable**: Ready-to-submit `submission.json`

---

## Success Metrics

### Baseline (No Evolution)
- **Target**: Pass@2 accuracy ‚â• 15% on held-out validation tasks
- **Rationale**: Random guessing = ~0%, simple heuristics = 5-10%

### With Persona Evolution (Stretch Goal)
- **Target**: Pass@2 accuracy ‚â• 20% on held-out validation tasks
- **Rationale**: Evolved prompts should improve over defaults

### Kaggle Submission
- **Target**: Public leaderboard score > 0 (beat null submission)
- **Stretch**: Top 50% of submissions

---

## Risks & Mitigations

### Risk 1: LLM fails to generate 5 diverse interpretations
**Mitigation**:
- Explicit prompt instruction: "All 5 personas must provide DIFFERENT perspectives"
- Post-processing: Filter duplicate interpretations, re-query if needed
- Fallback: Use single-persona analyst with multiple temperature settings

### Risk 2: Multi-solution programmer generates invalid Python
**Mitigation**:
- Strict output format with code fences
- Syntax validation before execution
- Error recovery: Skip broken solutions, continue with remaining

### Risk 3: Test-time ensemble too slow (>5 min per task)
**Mitigation**:
- Profile bottlenecks (likely API latency)
- Consider parallel API calls (if Gemini API supports)
- Optimize sandbox execution (cache compiled functions)

### Risk 4: Synthesis agent doesn't create truly diverse solution
**Mitigation**:
- Explicit constraint in prompt: "Use DIFFERENT algorithm than previous 5"
- Validation: Check code similarity metrics
- Fallback: Use 2nd-best solution from initial 5 instead of synthesis

---

## Open Questions

1. **Persona set size**: Is 5 optimal, or should we try 3 or 7?
2. **Temperature tuning**: What's the best temp for each agent?
3. **Validation set size**: 20 tasks sufficient for fitness evaluation?
4. **Evolution vs No Evolution**: Is default persona set good enough?

---

## Next Immediate Actions

1. ‚úÖ Save this plan (this file)
2. ‚è≥ Implement Multi-Persona Analyst (Task 1.1)
3. ‚è≥ Implement Multi-Solution Programmer (Task 1.2)
4. ‚è≥ Implement Synthesis Agent (Task 1.3)
5. ‚è≥ Build Test-Time Ensemble Pipeline (Task 1.4)
6. ‚è≥ Run validation on 10 tasks (Task 1.5)
7. ‚è≥ Measure baseline pass@2 accuracy
8. ‚è≥ Decide: Proceed with evolution or submit with defaults?

---

## References

- **ARC Prize 2025**: https://www.kaggle.com/competitions/arc-prize-2025
- **ARC-AGI Paper**: "On the Measure of Intelligence" by Fran√ßois Chollet
- **Original Project Plan**: `kickoff.md`
- **Phase 1 Plan**: `plan_20251024.md`
- **Prompt Analysis**: `prompt_analysis.md`
- **Situation Report**: `situation_20251102.md`

---

## Appendix: Default Persona Set

```python
DEFAULT_PERSONAS = {
    "persona_1": {
        "name": "Geometric Transformation Specialist",
        "emoji": "üî∑",
        "focus": "Rotations (90¬∞/180¬∞/270¬∞), reflections (horizontal/vertical), transpose, symmetry operations",
        "key_question": "Is this a spatial transformation of the grid itself?"
    },
    "persona_2": {
        "name": "Color Pattern Specialist",
        "emoji": "üé®",
        "focus": "Color changes, filling, replacement rules, color-based conditional logic",
        "key_question": "What color operations transform input to output?"
    },
    "persona_3": {
        "name": "Object Detection Specialist",
        "emoji": "üîç",
        "focus": "Shapes, connected regions, bounding boxes, object extraction/manipulation",
        "key_question": "Are there objects/regions to identify and manipulate?"
    },
    "persona_4": {
        "name": "Grid Structure Specialist",
        "emoji": "üìê",
        "focus": "Cropping, slicing, tiling, partitioning, resizing, concatenation, grid dimensions",
        "key_question": "Is this about grid dimensions or regional selection?"
    },
    "persona_5": {
        "name": "Logical Rules Specialist",
        "emoji": "‚ö°",
        "focus": "Conditional logic, counting, neighborhood analysis, cellular automata, pattern matching",
        "key_question": "Are there if-then rules based on positions, counts, or neighbors?"
    }
}
```

---

**End of Plan**
**Ready to proceed with implementation**: ‚úÖ
