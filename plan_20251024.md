# ARC-Prometheus Phase 1 Implementation Plan
**Date**: 2025-10-24
**Phase**: 1 - Core Prototype
**Goal**: Build minimal AI civilization ecosystem for end-to-end ARC task solving

---

## Configuration Decisions

- **Python Version**: 3.13
- **API Key Storage**: `.env` file in project root (with python-dotenv)
- **Dataset Location**: `data/` directory in project root (gitignored)
- **Testing Approach**: Focus on core functionality first, add comprehensive tests incrementally
- **PR Strategy**: Incremental PRs with automated pytest + demo script validation

---

## Project Structure

```
arc_prometheus/
├── README.md                    # Project overview and setup guide
├── .env.example                 # Template for API key
├── .env                        # Actual API key (gitignored)
├── .gitignore                  # Python, data, credentials
├── pyproject.toml              # Python 3.13+ project config
├── requirements.txt            # Core dependencies
├── data/                       # ARC dataset (gitignored)
│   └── arc-prize-2025/
│       ├── arc-agi_training_challenges.json
│       ├── arc-agi_training_solutions.json
│       ├── arc-agi_evaluation_challenges.json
│       ├── arc-agi_evaluation_solutions.json
│       ├── arc-agi_test_challenges.json
│       └── sample_submission.json
├── src/
│   └── arc_prometheus/
│       ├── __init__.py
│       ├── crucible/           # Sandbox environment
│       │   ├── __init__.py
│       │   ├── data_loader.py  # load_task, print_grid
│       │   ├── evaluator.py    # evaluate_grids
│       │   └── sandbox.py      # safe_execute with multiprocessing
│       ├── cognitive_cells/    # LLM agents
│       │   ├── __init__.py
│       │   ├── prompts.py      # Prompt templates
│       │   └── programmer.py   # generate_solver (Analyst+Programmer)
│       └── utils/
│           ├── __init__.py
│           └── config.py       # Load .env, API keys
├── tests/                      # Test-Driven Development
│   ├── __init__.py
│   ├── test_data_loader.py
│   ├── test_evaluator.py
│   ├── test_sandbox.py
│   └── test_programmer.py
└── scripts/
    ├── demo_phase1_1_data.py      # PR1: Demo data loading
    ├── demo_phase1_2_manual.py    # PR2: Demo manual solver
    ├── demo_phase1_3_llm.py       # PR3: Demo LLM generation
    └── run_phase1_test.py         # Final E2E pipeline
```

---

## Implementation Roadmap (Incremental PRs)

### **PR #1: Foundation & Data Infrastructure**
**Branch**: `feature/phase1-pr1-foundation`
**Scope**: Phase 1.1 + Phase 1.2 (Setup + Data/Evaluation)

#### Tasks

**1.1.1: Project Scaffolding**
- [ ] Create directory structure
- [ ] Create `.gitignore`:
  ```
  # Python
  __pycache__/
  *.py[cod]
  *$py.class
  .pytest_cache/

  # Virtual Environment
  venv/
  .venv/
  env/

  # Project Specific
  .env
  data/
  *.db
  *.sqlite

  # IDE
  .vscode/
  .idea/
  *.swp
  ```
- [ ] Create `pyproject.toml`:
  ```toml
  [project]
  name = "arc-prometheus"
  version = "0.1.0"
  description = "AI civilization for solving ARC Prize through evolutionary LLM agents"
  requires-python = ">=3.13"
  dependencies = [
      "numpy>=2.0.0",
      "google-generativeai>=0.8.0",
      "python-dotenv>=1.0.0",
  ]

  [project.optional-dependencies]
  dev = [
      "pytest>=8.0.0",
      "pytest-cov>=4.1.0",
  ]
  ```
- [ ] Create `requirements.txt`:
  ```
  numpy>=2.0.0
  google-generativeai>=0.8.0
  python-dotenv>=1.0.0
  pytest>=8.0.0
  pytest-cov>=4.1.0
  ```
- [ ] Create `.env.example`:
  ```
  # Get your API key from: https://makersuite.google.com/app/apikey
  GEMINI_API_KEY=your_api_key_here
  ```
- [ ] Add note: Get API key from https://makersuite.google.com/app/apikey

**1.1.2: Configuration Setup**
- [ ] Implement `src/arc_prometheus/utils/config.py`:
  ```python
  import os
  from dotenv import load_dotenv

  load_dotenv()

  GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")

  if not GEMINI_API_KEY:
      raise ValueError("GEMINI_API_KEY not found in .env file")
  ```

**1.2.1: Data Loader Implementation**
- [ ] Write tests: `tests/test_data_loader.py`
  - Test loading valid ARC JSON
  - Test file not found error
  - Test invalid JSON format
  - Test structure validation (train/test keys)
- [ ] Implement `src/arc_prometheus/crucible/data_loader.py`:
  ```python
  import json
  import numpy as np

  def load_task(filepath: str) -> dict[str, list[dict]]:
      """Load ARC task from JSON file.

      Args:
          filepath: Path to ARC task JSON file

      Returns:
          {"train": [...], "test": [...]}
          Each entry has "input" and "output" as 2D lists

      Raises:
          FileNotFoundError: If file doesn't exist
          ValueError: If JSON structure is invalid
      """

  def print_grid(grid: np.ndarray, label: str = "") -> None:
      """Print grid to console with color coding.

      Args:
          grid: 2D numpy array of integers (0-9)
          label: Optional label to print before grid
      """
  ```

**1.2.2: Evaluator Implementation**
- [ ] Write tests: `tests/test_evaluator.py`
  - Test identical grids (various sizes)
  - Test different values
  - Test shape mismatch
  - Test type handling (list vs ndarray)
- [ ] Implement `src/arc_prometheus/crucible/evaluator.py`:
  ```python
  import numpy as np

  def evaluate_grids(grid_a: np.ndarray, grid_b: np.ndarray) -> bool:
      """Check if two grids are identical.

      Args:
          grid_a: First grid (2D numpy array)
          grid_b: Second grid (2D numpy array)

      Returns:
          True if grids are identical in shape and values
      """
  ```

**1.2.3: Demo Script for PR1**
- [ ] Create `scripts/demo_phase1_1_data.py`:
  ```python
  # Load a sample ARC task
  # Print all train pairs with print_grid()
  # Evaluate train input vs expected output (should be different)
  # Print success message
  ```

**1.2.4: Documentation**
- [ ] Update README.md:
  - Project description from CLAUDE.md
  - Setup instructions (clone, pip install, .env setup)
  - Dataset placement instructions (copy to data/)
  - How to run demo: `python scripts/demo_phase1_1_data.py`
  - Expected output example

**PR1 Validation**:
```bash
# Tests pass
pytest tests/test_data_loader.py tests/test_evaluator.py -v

# Demo runs successfully
python scripts/demo_phase1_1_data.py
```

---

### **PR #2: Manual Solver & Sandbox Foundation**
**Branch**: `feature/phase1-pr2-sandbox`
**Scope**: Phase 1.3 + Phase 1.5 (Manual Test + Safe Execution)

#### Tasks

**1.3.1: Manual Solver Demo**
- [ ] Create `scripts/demo_phase1_2_manual.py`:
  - Choose simple ARC task (e.g., "copy input" or "fill with single color")
  - Implement `solve_manual(task_grid: np.ndarray) -> np.ndarray` using numpy
  - Load task with `load_task()`
  - Apply solver to all train pairs
  - Evaluate with `evaluate_grids()`
  - Print success/failure for each pair
  - **Success Criteria**: All train pairs pass

**1.5.1: Safe Execution Sandbox**
- [ ] Write tests: `tests/test_sandbox.py`
  - Test successful execution
  - Test timeout (5 seconds)
  - Test runtime exception (ZeroDivisionError, etc.)
  - Test syntax error handling
  - Test invalid return type
  - Test infinite loop timeout
- [ ] Implement `src/arc_prometheus/crucible/sandbox.py`:
  ```python
  import multiprocessing as mp
  from typing import Optional
  import numpy as np

  def safe_execute(
      solver_code: str,
      task_grid: np.ndarray,
      timeout: int = 5
  ) -> tuple[bool, Optional[np.ndarray]]:
      """Execute LLM-generated solver code safely in isolated process.

      Args:
          solver_code: Python code containing solve() function
          task_grid: Input grid to pass to solve()
          timeout: Maximum execution time in seconds

      Returns:
          (True, result_grid) on successful execution
          (False, None) on failure/timeout/exception

      Security:
          - Runs in isolated multiprocessing.Process
          - No filesystem access
          - No network access
          - Timeout enforcement
      """
  ```

**1.5.2: Sandbox Security Tests**
- [ ] Test filesystem access fails (try to write file)
- [ ] Test network access fails (try socket connection)
- [ ] Document security constraints in docstring

**1.5.3: Demo Script for PR2**
- [ ] Create `scripts/demo_phase1_2_sandbox.py`:
  ```python
  # Define sample solver code as string
  # Load simple ARC task
  # Use safe_execute() to run solver on train inputs
  # Compare outputs with evaluate_grids()
  # Print results
  # Demonstrate timeout with intentional infinite loop
  ```

**PR2 Validation**:
```bash
# Tests pass
pytest tests/test_sandbox.py -v

# Manual solver demo passes
python scripts/demo_phase1_2_manual.py

# Sandbox demo passes
python scripts/demo_phase1_2_sandbox.py
```

---

### **PR #3: LLM Integration & End-to-End Pipeline**
**Branch**: `feature/phase1-pr3-llm-e2e`
**Scope**: Phase 1.4 + Phase 1.6 (LLM Generation + Complete Pipeline)

#### Tasks

**1.4.1: Prompt Design**
- [ ] Implement `src/arc_prometheus/cognitive_cells/prompts.py`:
  ```python
  def create_solver_prompt(train_pairs: list[dict]) -> str:
      """Create prompt for Gemini to generate solver code.

      Args:
          train_pairs: List of {"input": [[...]], "output": [[...]]}

      Returns:
          Formatted prompt string with:
          - ASCII art grid visualization
          - Analysis request
          - Code generation request
          - Constraints (numpy only)
      """
  ```

**1.4.2: Code Extraction Parser**
- [ ] Write tests: `tests/test_programmer.py`
  - Test extracting code from ```python ... ``` blocks
  - Test handling raw code (no delimiters)
  - Test handling multiple code blocks
  - Test handling markdown formatting
  - Test extracting only solve() function
- [ ] Implement parser in `cognitive_cells/programmer.py`:
  ```python
  def extract_code_from_response(response_text: str) -> str:
      """Extract Python code from LLM response.

      Handles:
          - Code in ```python blocks
          - Raw code without delimiters
          - Multiple code blocks (extract solve function)
          - Markdown formatting artifacts
      """
  ```

**1.4.3: LLM Solver Generation**
- [ ] Implement `src/arc_prometheus/cognitive_cells/programmer.py`:
  ```python
  import google.generativeai as genai
  from ..utils.config import GEMINI_API_KEY

  def generate_solver(train_pairs: list[dict]) -> str:
      """Generate solver code using Gemini API.

      Args:
          train_pairs: List of input/output examples

      Returns:
          Python code string containing solve() function

      Raises:
          APIError: If Gemini API call fails
          ValueError: If response cannot be parsed
      """
  ```

**1.4.4: LLM Integration Demo**
- [ ] Create `scripts/demo_phase1_3_llm.py`:
  ```python
  # Load simple ARC task
  # Call generate_solver(train_pairs)
  # Print generated code
  # Use safe_execute() on train pairs
  # Evaluate results
  # Print success/failure
  ```

**1.6.1: End-to-End Pipeline**
- [ ] Implement `scripts/run_phase1_test.py`:
  ```python
  # Usage: python scripts/run_phase1_test.py data/arc-prize-2024/task_xyz.json

  # 1. Load ARC task from filepath argument
  # 2. Call generate_solver(train_pairs)
  # 3. For each train pair:
  #    - Execute solver with safe_execute()
  #    - Evaluate result with evaluate_grids()
  # 4. Print detailed results:
  #    - Generated code
  #    - Train accuracy (X/Y correct)
  #    - Individual failures with diff
  # 5. Save successful solver to file
  ```

**1.6.2: E2E Testing**
- [ ] Test with 3 simple ARC tasks
- [ ] Document success rate
- [ ] Save example successful solver
- [ ] Add troubleshooting notes for common failures

**1.6.3: Final Documentation**
- [ ] Update README.md:
  - Phase 1 completion status
  - Example E2E run with output
  - Success criteria achieved
  - Known limitations
  - Next steps (Phase 2 preview)

**PR3 Validation**:
```bash
# Tests pass
pytest tests/test_programmer.py -v

# LLM demo runs (requires API key)
python scripts/demo_phase1_3_llm.py

# E2E pipeline runs successfully
python scripts/run_phase1_test.py data/arc-prize-2025/arc-agi_training_challenges.json <task_id>
```

---

## Success Criteria

### **PR1 Complete**:
- ✅ Data loader reads ARC JSON files
- ✅ Grid printing displays correctly
- ✅ Grid evaluation compares accurately
- ✅ Demo script runs without errors
- ✅ Tests pass

### **PR2 Complete**:
- ✅ Manual solver solves simple ARC task (all train pairs)
- ✅ Sandbox executes code safely with timeout
- ✅ Security constraints prevent filesystem/network access
- ✅ Demo scripts run successfully
- ✅ Tests pass

### **PR3 Complete** (Phase 1 Milestone):
- ✅ Gemini API generates solver code
- ✅ Code parser handles various response formats
- ✅ LLM-generated solver solves ≥1 train pair
- ✅ E2E pipeline runs end-to-end
- ✅ Success documented with examples
- ✅ Tests pass

### **Overall Phase 1 Success**:
- ✅ "First Victory": AI-generated code successfully solves at least one ARC train pair
- ✅ All infrastructure validated (data, evaluation, sandbox, LLM)
- ✅ Codebase ready for Phase 2 (evolutionary loop)

---

## Testing Strategy

### Core Functionality Focus
- **PR1**: Basic data operations tests
- **PR2**: Sandbox security and timeout tests
- **PR3**: Code parsing tests, LLM integration manual smoke test

### Test Execution
```bash
# Run all tests
pytest tests/ -v

# Run with coverage (target >80% for core modules)
pytest tests/ --cov=src/arc_prometheus --cov-report=term-missing

# Run specific module tests
pytest tests/test_data_loader.py -v
```

---

## Commit Strategy

### PR1 Commits
1. `feat: add project structure and configuration (Phase 1.1)`
2. `test: add data loader and evaluator tests`
3. `feat: implement data loader and evaluator (Phase 1.2)`
4. `feat: add data loading demo script`
5. `docs: update README with setup and demo instructions`

### PR2 Commits
1. `feat: add manual solver demo (Phase 1.3)`
2. `test: add sandbox execution tests`
3. `feat: implement safe execution sandbox (Phase 1.5)`
4. `feat: add sandbox demo script`
5. `docs: update README with manual solver results`

### PR3 Commits
1. `feat: add LLM prompt templates (Phase 1.4)`
2. `test: add code extraction parser tests`
3. `feat: implement Gemini programmer agent`
4. `feat: add LLM generation demo script`
5. `feat: implement E2E pipeline (Phase 1.6)`
6. `docs: update README with Phase 1 completion and results`

---

## Environment Setup Guide

### For User (First Time)
```bash
# 1. Clone and navigate
cd arc_prometheus

# 2. Create virtual environment
python3.13 -m venv venv
source venv/bin/activate  # On Mac/Linux

# 3. Install dependencies
pip install -r requirements.txt

# 4. Set up API key
cp .env.example .env
# Edit .env and add your GEMINI_API_KEY

# 5. Place ARC dataset
# Copy your arc-prize-2025 data to: data/arc-prize-2025/
# Download from: https://www.kaggle.com/competitions/arc-prize-2025/data

# 6. Verify setup
python -c "from src.arc_prometheus.utils.config import GEMINI_API_KEY; print('API key loaded!')"
```

---

## Next Steps After Phase 1

Once Phase 1 is complete, we'll have:
- ✅ Working data infrastructure
- ✅ Safe code execution sandbox
- ✅ LLM-based solver generation
- ✅ End-to-end validation

**Phase 2 Preview** (Future):
- Fitness function implementation
- Refiner agent for debugging
- Evolution loop with multiple generations
- Test accuracy tracking

---

## Notes

- **TDD Approach**: Tests written before implementation where feasible
- **Incremental Validation**: Each PR must have runnable demo
- **Security First**: Sandbox isolation is non-negotiable
- **LLM Robustness**: Parser must handle real-world LLM response variations
- **Documentation**: Keep README updated with current capabilities
