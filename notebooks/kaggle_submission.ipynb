{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ARC Prize 2025 Submission: AI Civilization (Offline Inference)\n\n**Strategy:** Multi-agent evolution with local Code Gemma 7B (no internet access)\n\n**Components:**\n- **Analyst**: Analyzes ARC tasks to infer transformation patterns\n- **Programmer**: Generates solver code based on analysis\n- **SimplifiedEvolution**: Minimal evolution loop for time constraints\n\n**Constraints:**\n- No internet access (offline inference only)\n- 12-hour runtime for 240 tasks (~3 min/task)\n- pass@2 format (2 diverse attempts per test input)\n\n**Requirements:**\n- Python 3.9+ (uses modern type hints: `list[dict]`, `tuple[bool, ...]`)\n- Kaggle GPU: L4x4 (96GB VRAM) for Code Gemma 7B\n- Code Gemma 7B model uploaded as Kaggle dataset (~16GB)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Environment Setup\n",
    "import json\n",
    "import multiprocessing\n",
    "import re\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Only import transformers if running on Kaggle (has GPU)\n",
    "# For local testing without transformers, we'll use a mock\n",
    "try:\n",
    "    import torch\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "    HAS_TRANSFORMERS = True\n",
    "except ImportError:\n",
    "    print(\"Warning: transformers not available (local testing mode)\")\n",
    "    HAS_TRANSFORMERS = False\n",
    "\n",
    "print(\"Environment setup complete!\")\n",
    "print(f\"Transformers available: {HAS_TRANSFORMERS}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load Local Model (No Internet!)\n",
    "\n",
    "# Path to Code Gemma model (uploaded as Kaggle dataset)\n",
    "MODEL_PATH = \"/kaggle/input/codegemma-7b/\"  # On Kaggle\n",
    "# MODEL_PATH = \"./models/codegemma-7b/\"  # For local testing\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "if HAS_TRANSFORMERS:\n",
    "    print(\"Loading Code Gemma 7B model...\")\n",
    "    try:\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            MODEL_PATH,\n",
    "            device_map=\"auto\",\n",
    "            torch_dtype=torch.float16,  # Memory optimization\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "        print(f\"✅ Model loaded successfully! Device: {model.device}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ ERROR: Model loading failed: {e}\")\n",
    "        print(\"This will cause the notebook to fail. Please check:\")\n",
    "        print(\"1. Model files exist in Kaggle dataset\")\n",
    "        print(\"2. Sufficient disk space (~16GB)\")\n",
    "        print(\"3. Dataset path is correct\")\n",
    "        raise  # Don't continue with mock model on Kaggle\n",
    "else:\n",
    "    print(\"⚠️  WARNING: Running in LOCAL TESTING mode with mock responses\")\n",
    "    print(\"This is for validation only - Kaggle submission requires the real model\")\n",
    "\n",
    "\n",
    "def generate_with_local_model(\n",
    "    prompt: str, temperature: float = 0.3, max_tokens: int = 2048\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Local inference - completely offline (no API calls).\n",
    "\n",
    "    Args:\n",
    "        prompt: Input prompt for the model\n",
    "        temperature: Sampling temperature (higher = more creative)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "\n",
    "    Returns:\n",
    "        Generated text from the model\n",
    "    \"\"\"\n",
    "    if not HAS_TRANSFORMERS or model is None:\n",
    "        # Mock response for local testing\n",
    "        return \"def solve(task_grid: np.ndarray) -> np.ndarray:\\n    return task_grid\"\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        prompt, return_tensors=\"pt\", truncation=True, max_length=4096\n",
    "    ).to(model.device)\n",
    "\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        temperature=temperature,\n",
    "        max_new_tokens=max_tokens,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "print(\"Model inference function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Core Helper Functions (Duplicated from src/ for offline compatibility)\n",
    "\n",
    "\n",
    "def format_grid(grid: list[list[int]]) -> str:\n",
    "    \"\"\"\n",
    "    Convert grid to ASCII art for LLM prompt.\n",
    "\n",
    "    Example:\n",
    "        [[0, 1], [2, 3]] -> \"0 1\\n2 3\"\n",
    "    \"\"\"\n",
    "    return \"\\n\".join(\" \".join(str(cell) for cell in row) for row in grid)\n",
    "\n",
    "\n",
    "def format_examples(examples: list[dict]) -> str:\n",
    "    \"\"\"\n",
    "    Format training examples for LLM prompt.\n",
    "\n",
    "    Args:\n",
    "        examples: List of {\"input\": grid, \"output\": grid} dicts\n",
    "\n",
    "    Returns:\n",
    "        Formatted string with all examples\n",
    "    \"\"\"\n",
    "    formatted = []\n",
    "    for i, ex in enumerate(examples):\n",
    "        formatted.append(f\"Example {i + 1}:\")\n",
    "        formatted.append(f\"Input:\\n{format_grid(ex['input'])}\")\n",
    "        formatted.append(f\"Output:\\n{format_grid(ex['output'])}\")\n",
    "        formatted.append(\"\")\n",
    "    return \"\\n\".join(formatted)\n",
    "\n",
    "\n",
    "def extract_solve_function(llm_response: str) -> str:\n",
    "    \"\"\"\n",
    "    Extract solve() function from LLM output.\n",
    "    Handles markdown code blocks and plain code.\n",
    "\n",
    "    Args:\n",
    "        llm_response: Raw LLM output\n",
    "\n",
    "    Returns:\n",
    "        Extracted Python code\n",
    "    \"\"\"\n",
    "    # Try to extract from markdown code block first\n",
    "    code_block_pattern = r\"```python\\n(.+?)```\"\n",
    "    match = re.search(code_block_pattern, llm_response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    # Try without language specifier\n",
    "    code_block_pattern = r\"```\\n(.+?)```\"\n",
    "    match = re.search(code_block_pattern, llm_response, re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    # If no code block, try to find def solve(\n",
    "    if \"def solve(\" in llm_response:\n",
    "        # Extract from def solve( to end\n",
    "        start_idx = llm_response.find(\"def solve(\")\n",
    "        return llm_response[start_idx:].strip()\n",
    "\n",
    "    # Return as-is (might be pure code)\n",
    "    return llm_response.strip()\n",
    "\n",
    "\n",
    "def execute_solver_safe(\n",
    "    code: str, input_grid: np.ndarray, timeout: int = 5\n",
    ") -> tuple[bool, np.ndarray | None, dict | None]:\n",
    "    \"\"\"\n",
    "    Execute solver code with timeout and error handling.\n",
    "    Uses multiprocessing for isolation.\n",
    "\n",
    "    SECURITY NOTE: This sandbox provides:\n",
    "    - Process isolation (timeout, crash protection)\n",
    "    - Restricted builtins (no eval, exec, compile, open, __import__)\n",
    "\n",
    "    LIMITATIONS (acceptable for Kaggle's isolated environment):\n",
    "    - Cannot prevent filesystem access via numpy/ctypes\n",
    "    - Cannot prevent network access if libraries are available\n",
    "    - Kaggle provides container-level isolation for these\n",
    "\n",
    "    For production use outside Kaggle, use Docker sandbox instead.\n",
    "\n",
    "    Args:\n",
    "        code: Solver code string\n",
    "        input_grid: Input grid to transform\n",
    "        timeout: Timeout in seconds\n",
    "\n",
    "    Returns:\n",
    "        (success, result_grid, error_detail)\n",
    "    \"\"\"\n",
    "\n",
    "    def _run_solver(code_str, task_grid, result_queue):\n",
    "        \"\"\"Worker function for multiprocess execution\"\"\"\n",
    "        try:\n",
    "            # Create restricted builtins (remove dangerous functions)\n",
    "            # Per CLAUDE.md: \"Restricted builtins: eval, exec, compile, open removed\"\n",
    "            safe_builtins = {\n",
    "                k: v\n",
    "                for k, v in __builtins__.items()\n",
    "                if k not in [\"eval\", \"exec\", \"compile\", \"open\", \"__import__\"]\n",
    "            }\n",
    "\n",
    "            # Create restricted namespace\n",
    "            namespace = {\n",
    "                \"__builtins__\": safe_builtins,\n",
    "                \"np\": np,\n",
    "                \"task_grid\": task_grid,\n",
    "            }\n",
    "\n",
    "            # Execute code with restricted namespace\n",
    "            exec(code_str, namespace)  # noqa: S102\n",
    "\n",
    "            # Call solve function\n",
    "            if \"solve\" not in namespace:\n",
    "                result_queue.put(\n",
    "                    (\n",
    "                        False,\n",
    "                        None,\n",
    "                        {\n",
    "                            \"error_type\": \"missing_function\",\n",
    "                            \"error_message\": \"No solve() function found\",\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "                return\n",
    "\n",
    "            result = namespace[\"solve\"](task_grid)\n",
    "\n",
    "            # Validate result\n",
    "            if not isinstance(result, np.ndarray):\n",
    "                result_queue.put(\n",
    "                    (\n",
    "                        False,\n",
    "                        None,\n",
    "                        {\n",
    "                            \"error_type\": \"invalid_return\",\n",
    "                            \"error_message\": f\"Expected np.ndarray, got {type(result)}\",\n",
    "                        },\n",
    "                    )\n",
    "                )\n",
    "                return\n",
    "\n",
    "            result_queue.put((True, result, None))\n",
    "\n",
    "        except Exception as e:\n",
    "            result_queue.put(\n",
    "                (False, None, {\"error_type\": type(e).__name__, \"error_message\": str(e)})\n",
    "            )\n",
    "\n",
    "    # Run in separate process with resource cleanup\n",
    "    result_queue = multiprocessing.Queue()\n",
    "    process = multiprocessing.Process(\n",
    "        target=_run_solver, args=(code, input_grid, result_queue)\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        process.start()\n",
    "        process.join(timeout=timeout)\n",
    "\n",
    "        if process.is_alive():\n",
    "            # Timeout - terminate process\n",
    "            process.terminate()\n",
    "            process.join()\n",
    "            return (\n",
    "                False,\n",
    "                None,\n",
    "                {\n",
    "                    \"error_type\": \"timeout\",\n",
    "                    \"error_message\": f\"Execution exceeded {timeout}s\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "        if result_queue.empty():\n",
    "            return (\n",
    "                False,\n",
    "                None,\n",
    "                {\n",
    "                    \"error_type\": \"unknown\",\n",
    "                    \"error_message\": \"Process terminated without result\",\n",
    "                },\n",
    "            )\n",
    "\n",
    "        return result_queue.get()\n",
    "\n",
    "    finally:\n",
    "        # Clean up resources\n",
    "        result_queue.close()\n",
    "        result_queue.join_thread()\n",
    "\n",
    "\n",
    "def calculate_fitness(code: str, task: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Calculate fitness score for solver code.\n",
    "    Fitness = (train_correct * 1) + (test_correct * 10)\n",
    "\n",
    "    Args:\n",
    "        code: Solver code string\n",
    "        task: Task dict with \"train\" and \"test\" examples\n",
    "\n",
    "    Returns:\n",
    "        {\"fitness\": score, \"train_correct\": count, \"test_correct\": count}\n",
    "    \"\"\"\n",
    "    train_correct = 0\n",
    "    test_correct = 0\n",
    "\n",
    "    # Evaluate on training examples\n",
    "    for example in task.get(\"train\", []):\n",
    "        input_grid = np.array(example[\"input\"], dtype=np.int64)\n",
    "        expected_output = np.array(example[\"output\"], dtype=np.int64)\n",
    "\n",
    "        success, result, _ = execute_solver_safe(code, input_grid)\n",
    "\n",
    "        if success and np.array_equal(result, expected_output):\n",
    "            train_correct += 1\n",
    "\n",
    "    # Evaluate on test examples (if available)\n",
    "    for example in task.get(\"test\", []):\n",
    "        if \"output\" not in example:\n",
    "            continue  # No ground truth for this test\n",
    "\n",
    "        input_grid = np.array(example[\"input\"], dtype=np.int64)\n",
    "        expected_output = np.array(example[\"output\"], dtype=np.int64)\n",
    "\n",
    "        success, result, _ = execute_solver_safe(code, input_grid)\n",
    "\n",
    "        if success and np.array_equal(result, expected_output):\n",
    "            test_correct += 1\n",
    "\n",
    "    fitness = (train_correct * 1) + (test_correct * 10)\n",
    "\n",
    "    return {\n",
    "        \"fitness\": fitness,\n",
    "        \"train_correct\": train_correct,\n",
    "        \"test_correct\": test_correct,\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"Helper functions ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Simplified AI Agents (Offline versions)\n",
    "\n",
    "\n",
    "class OfflineAnalyst:\n",
    "    \"\"\"\n",
    "    Simplified Analyst for Kaggle (no API, uses local model).\n",
    "    Analyzes ARC tasks to infer transformation patterns.\n",
    "    \"\"\"\n",
    "\n",
    "    def analyze_task(self, task_json: dict) -> str:\n",
    "        \"\"\"\n",
    "        Analyze task and generate natural language specification.\n",
    "\n",
    "        Args:\n",
    "            task_json: Task dict with \"train\" examples\n",
    "\n",
    "        Returns:\n",
    "            Analysis string with pattern description\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Analyze this ARC puzzle and describe the transformation pattern.\n",
    "\n",
    "Training examples:\n",
    "{format_examples(task_json[\"train\"])}\n",
    "\n",
    "Provide a concise analysis:\n",
    "1. PATTERN: One sentence describing the transformation rule\n",
    "2. OBSERVATIONS: Key features (colors, shapes, positions)\n",
    "3. APPROACH: High-level implementation strategy\n",
    "\n",
    "Be specific and actionable.\"\"\"\n",
    "\n",
    "        return generate_with_local_model(prompt, temperature=0.3)\n",
    "\n",
    "\n",
    "class OfflineProgrammer:\n",
    "    \"\"\"\n",
    "    Simplified Programmer for Kaggle.\n",
    "    Generates solver code based on task examples and optional analysis.\n",
    "    \"\"\"\n",
    "\n",
    "    def generate_solver(self, task_json: dict, analyst_spec: str = None) -> str:\n",
    "        \"\"\"\n",
    "        Generate solver code for the task.\n",
    "\n",
    "        Args:\n",
    "            task_json: Task dict with \"train\" examples\n",
    "            analyst_spec: Optional analysis from Analyst\n",
    "\n",
    "        Returns:\n",
    "            Python code string with solve() function\n",
    "        \"\"\"\n",
    "        analysis_section = (\n",
    "            f\"\\n\\nAnalyst's observations:\\n{analyst_spec}\" if analyst_spec else \"\"\n",
    "        )\n",
    "\n",
    "        prompt = f\"\"\"Generate a Python function to solve this ARC puzzle.{analysis_section}\n",
    "\n",
    "Training examples:\n",
    "{format_examples(task_json[\"train\"])}\n",
    "\n",
    "Requirements:\n",
    "- Function signature: def solve(task_grid: np.ndarray) -> np.ndarray:\n",
    "- Use only numpy for array operations (imported as np)\n",
    "- Return the transformed grid as np.ndarray\n",
    "- Grid values are integers 0-9 (colors)\n",
    "\n",
    "Generate ONLY the solve() function code (no explanations):\"\"\"\n",
    "\n",
    "        response = generate_with_local_model(prompt, temperature=0.4, max_tokens=1024)\n",
    "        return extract_solve_function(response)\n",
    "\n",
    "\n",
    "class SimplifiedEvolution:\n",
    "    \"\"\"\n",
    "    Minimal evolution loop for Kaggle time constraints.\n",
    "    Simplified version of PopulationEvolution for offline inference.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, population_size: int = 5, max_generations: int = 3):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            population_size: Number of solvers to generate\n",
    "            max_generations: Number of evolution iterations (unused in simplified version)\n",
    "        \"\"\"\n",
    "        self.population_size = population_size\n",
    "        self.max_generations = max_generations\n",
    "        self.analyst = OfflineAnalyst()\n",
    "        self.programmer = OfflineProgrammer()\n",
    "\n",
    "    def evolve(self, task: dict) -> list[dict]:\n",
    "        \"\"\"\n",
    "        Evolve population of solvers for the task.\n",
    "\n",
    "        Args:\n",
    "            task: Task dict with \"train\" and \"test\" examples\n",
    "\n",
    "        Returns:\n",
    "            List of solver dicts sorted by fitness (best first)\n",
    "        \"\"\"\n",
    "        # 1. Analyst analyzes task\n",
    "        analysis = self.analyst.analyze_task(task)\n",
    "\n",
    "        # 2. Generate initial population\n",
    "        population = []\n",
    "        for i in range(self.population_size):\n",
    "            try:\n",
    "                code = self.programmer.generate_solver(task, analysis)\n",
    "                fitness_result = calculate_fitness(code, task)\n",
    "                population.append(\n",
    "                    {\n",
    "                        \"code\": code,\n",
    "                        \"fitness\": fitness_result[\"fitness\"],\n",
    "                        \"train_correct\": fitness_result[\"train_correct\"],\n",
    "                        \"test_correct\": fitness_result[\"test_correct\"],\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                # Skip failed generations\n",
    "                print(f\"Warning: Failed to generate solver {i}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # 3. Sort by fitness (best first)\n",
    "        population.sort(key=lambda x: x[\"fitness\"], reverse=True)\n",
    "\n",
    "        return population\n",
    "\n",
    "\n",
    "print(\"AI agents ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Load Test Data and Run Inference\n",
    "\n",
    "# Path to test data (provided by Kaggle)\n",
    "TEST_DATA_PATH = \"/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json\"\n",
    "# TEST_DATA_PATH = \"../data/arc-prize-2025/arc-agi_evaluation_challenges_merged.json\"  # For local testing\n",
    "\n",
    "print(f\"Loading test tasks from: {TEST_DATA_PATH}\")\n",
    "\n",
    "try:\n",
    "    with open(TEST_DATA_PATH) as f:\n",
    "        test_tasks = json.load(f)\n",
    "    print(f\"Loaded {len(test_tasks)} test tasks\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: Test data not found (using mock for local testing)\")\n",
    "    test_tasks = {}\n",
    "\n",
    "# Initialize evolution engine\n",
    "evolution_engine = SimplifiedEvolution(\n",
    "    population_size=5,  # Small population for time constraints\n",
    "    max_generations=3,  # Few generations for speed\n",
    ")\n",
    "\n",
    "# Track progress\n",
    "submission = {}\n",
    "start_time = time.time()\n",
    "\n",
    "print(f\"Starting inference on {len(test_tasks)} tasks...\")\n",
    "print(\n",
    "    f\"Estimated time: {len(test_tasks) * 3} minutes ({len(test_tasks) * 3 / 60:.1f} hours)\"\n",
    ")\n",
    "\n",
    "for idx, (task_id, task) in enumerate(test_tasks.items()):\n",
    "    task_start = time.time()\n",
    "\n",
    "    print(f\"\\nProcessing {idx + 1}/{len(test_tasks)}: {task_id}\")\n",
    "\n",
    "    try:\n",
    "        # Evolve population\n",
    "        population = evolution_engine.evolve(task)\n",
    "\n",
    "        # Select best 2 for pass@2\n",
    "        if len(population) == 0:\n",
    "            # Fallback: use dummy solver\n",
    "            best = {\n",
    "                \"code\": \"def solve(task_grid: np.ndarray) -> np.ndarray:\\n    return task_grid\"\n",
    "            }\n",
    "            second = best\n",
    "        elif len(population) == 1:\n",
    "            best = population[0]\n",
    "            second = best\n",
    "        else:\n",
    "            best = population[0]\n",
    "            second = population[1]\n",
    "\n",
    "        # Generate predictions for all test inputs\n",
    "        predictions = []\n",
    "        for test_input in task.get(\"test\", []):\n",
    "            input_grid = np.array(test_input[\"input\"], dtype=np.int64)\n",
    "\n",
    "            # Attempt 1 (best solver) - use input_grid as fallback for correct dimensions\n",
    "            success1, pred1, _ = execute_solver_safe(\n",
    "                best[\"code\"], input_grid, timeout=3\n",
    "            )\n",
    "            attempt_1 = pred1.tolist() if success1 else input_grid.tolist()\n",
    "\n",
    "            # Attempt 2 (second-best solver) - use input_grid as fallback for correct dimensions\n",
    "            success2, pred2, _ = execute_solver_safe(\n",
    "                second[\"code\"], input_grid, timeout=3\n",
    "            )\n",
    "            attempt_2 = pred2.tolist() if success2 else input_grid.tolist()\n",
    "\n",
    "            predictions.append({\"attempt_1\": attempt_1, \"attempt_2\": attempt_2})\n",
    "\n",
    "        submission[task_id] = predictions\n",
    "\n",
    "        task_time = time.time() - task_start\n",
    "        print(\n",
    "            f\"  Completed in {task_time:.1f}s (best fitness: {best.get('fitness', 0)})\"\n",
    "        )\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR: {e}\")\n",
    "        # Fallback: use input grids to ensure correct dimensions\n",
    "        predictions = [\n",
    "            {\n",
    "                \"attempt_1\": test_input[\"input\"],\n",
    "                \"attempt_2\": test_input[\"input\"],\n",
    "            }\n",
    "            for test_input in task.get(\"test\", [])\n",
    "        ]\n",
    "        submission[task_id] = predictions\n",
    "\n",
    "    # Progress update every 10 tasks\n",
    "    if (idx + 1) % 10 == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        avg_time = elapsed / (idx + 1)\n",
    "        remaining = avg_time * (len(test_tasks) - idx - 1)\n",
    "        print(f\"\\nProgress: {idx + 1}/{len(test_tasks)} tasks\")\n",
    "        print(\n",
    "            f\"Elapsed: {elapsed / 60:.1f} min | Avg: {avg_time:.1f}s/task | ETA: {remaining / 60:.1f} min\"\n",
    "        )\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "print(\"\\nInference complete!\")\n",
    "print(f\"Total time: {total_time / 60:.1f} minutes ({total_time / 3600:.2f} hours)\")\n",
    "print(f\"Average: {total_time / len(test_tasks):.1f} seconds/task\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Save Submission\n",
    "\n",
    "OUTPUT_PATH = \"submission.json\"\n",
    "\n",
    "print(f\"Saving submission to: {OUTPUT_PATH}\")\n",
    "\n",
    "with open(OUTPUT_PATH, \"w\") as f:\n",
    "    json.dump(submission, f, indent=2)\n",
    "\n",
    "print(\"Submission saved successfully!\")\n",
    "print(f\"Tasks: {len(submission)}\")\n",
    "print(\"Format: pass@2 (2 attempts per test input)\")\n",
    "\n",
    "# Validate submission format\n",
    "valid = True\n",
    "for task_id, predictions in submission.items():\n",
    "    if not isinstance(predictions, list):\n",
    "        print(f\"ERROR: {task_id} has invalid predictions type\")\n",
    "        valid = False\n",
    "        continue\n",
    "\n",
    "    for pred in predictions:\n",
    "        if not isinstance(pred, dict):\n",
    "            print(f\"ERROR: {task_id} has invalid prediction dict\")\n",
    "            valid = False\n",
    "            break\n",
    "\n",
    "        if \"attempt_1\" not in pred or \"attempt_2\" not in pred:\n",
    "            print(f\"ERROR: {task_id} missing attempt_1 or attempt_2\")\n",
    "            valid = False\n",
    "            break\n",
    "\n",
    "if valid:\n",
    "    print(\"\\n✅ Submission format validated successfully!\")\n",
    "else:\n",
    "    print(\"\\n❌ Submission format validation FAILED!\")\n",
    "\n",
    "print(\"\\nReady for Kaggle submission!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
