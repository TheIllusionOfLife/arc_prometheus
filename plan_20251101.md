# Development Plan: Kaggle Offline Inference (Phase 4a)

**Date:** November 01, 2025
**Status:** Active Development
**Competition Deadline:** November 3, 2025

---

## Critical Correction from plan_20251031.md

**What Was Wrong:**
- plan_20251031.md **Task 3.6: Kaggle Baseline Submission** assumed we could call Gemini API on Kaggle ❌
- The plan showed: `from arc_prometheus.cognitive_cells import Analyst, Programmer...` with NO mention of offline constraints
- **Reality**: Kaggle is a CODE COMPETITION with **NO INTERNET ACCESS** during inference
- Cannot call Gemini/GPT/Claude APIs
- Must use locally-available models (Gemma/Code Gemma)
- Notebook must be self-contained and run offline on private test set

**The Truth About Kaggle Submission:**
1. **Submission Format**: Kaggle Notebook (.ipynb), NOT CSV
2. **Internet Access**: ❌ NONE (cannot call any external APIs)
3. **GPU Hardware**: L4x4 (96GB VRAM) - can run 13B-27B models
4. **Max Runtime**: 12 hours for 240 tasks (~3 min/task)
5. **Private Test Set**: Inference runs on unseen test data (no test outputs provided)

---

## Phase 4a: Kaggle Offline Inference (3-4 days)

**Goal:** Create standalone Kaggle notebook that reimplements AI Civilization workflow using local Gemma/Code Gemma models (no internet, no Gemini API).

**Strategy:** Keep existing Gemini pipeline intact (for development/refinement). Create separate offline notebook for Kaggle submission.

---

### Task 1: Create Standalone Kaggle Notebook (6-8 hours)

**Create:** `notebooks/kaggle_submission.ipynb`

**Self-contained structure (no src/ dependencies):**

#### Cell 1: Environment Setup
```python
import sys
import json
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import multiprocessing
from typing import Tuple, Optional
```

#### Cell 2: Load Local Model (No Internet!)
```python
# Load Code Gemma 7B from uploaded Kaggle dataset
MODEL_PATH = "/kaggle/input/codegemma-7b/"

model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    torch_dtype=torch.float16  # Memory optimization
)
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)

def generate_with_local_model(prompt: str, temperature: float = 0.3, max_tokens: int = 2048) -> str:
    """Local inference - completely offline"""
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4096).to(model.device)
    outputs = model.generate(
        **inputs,
        temperature=temperature,
        max_new_tokens=max_tokens,
        do_sample=True,
        pad_token_id=tokenizer.eos_token_id
    )
    return tokenizer.decode(outputs[0], skip_special_tokens=True)
```

#### Cell 3: Core Helper Functions (Copy from existing code, simplified)
```python
# 1. Task formatting (ASCII art for prompts)
def format_grid(grid: list) -> str:
    """Convert grid to ASCII art for LLM prompt"""
    # Implementation copied from existing code

# 2. Safe execution (multiprocess sandbox)
def execute_solver_safe(code: str, input_grid: np.ndarray, timeout: int = 5) -> Tuple[bool, Optional[np.ndarray], Optional[dict]]:
    """Execute solver with timeout and error handling"""
    # Implementation copied from src/arc_prometheus/crucible/sandbox.py
    # Uses multiprocessing for isolation

# 3. Fitness calculation
def calculate_fitness(code: str, task: dict) -> dict:
    """Calculate fitness: train_correct * 1 + test_correct * 10"""
    # Implementation copied from src/arc_prometheus/evolutionary_engine/fitness.py

# 4. Code parsing
def extract_solve_function(llm_response: str) -> str:
    """Extract solve() function from LLM output (handle markdown)"""
    # Implementation copied from src/arc_prometheus/cognitive_cells/programmer.py
```

#### Cell 4: Simplified AI Agents (Offline versions)
```python
class OfflineAnalyst:
    """Simplified Analyst for Kaggle (no API, uses local model)"""
    def analyze_task(self, task_json: dict) -> str:
        prompt = f"""Analyze this ARC puzzle and describe the transformation pattern.

Training examples:
{format_examples(task_json["train"])}

Provide:
1. PATTERN: One sentence describing the transformation rule
2. OBSERVATIONS: Key features (colors, shapes, positions)
3. APPROACH: High-level implementation strategy

Be concise and specific."""

        return generate_with_local_model(prompt, temperature=0.3)

class OfflineProgrammer:
    """Simplified Programmer for Kaggle"""
    def generate_solver(self, task_json: dict, analyst_spec: str = None) -> str:
        prompt = f"""Generate a Python function to solve this ARC puzzle.

{f'Analysis: {analyst_spec}' if analyst_spec else ''}

Training examples:
{format_examples(task_json["train"])}

Requirements:
- Function signature: def solve(task_grid: np.ndarray) -> np.ndarray:
- Use only numpy for array operations
- Return the transformed grid

Generate ONLY the solve() function code:"""

        response = generate_with_local_model(prompt, temperature=0.4, max_tokens=1024)
        return extract_solve_function(response)

class SimplifiedEvolution:
    """Minimal evolution loop for Kaggle time constraints"""
    def __init__(self, population_size: int = 5, max_generations: int = 3):
        self.population_size = population_size
        self.max_generations = max_generations
        self.analyst = OfflineAnalyst()
        self.programmer = OfflineProgrammer()

    def evolve(self, task: dict) -> list:
        """Evolve population of solvers"""
        # 1. Analyst analyzes task
        analysis = self.analyst.analyze_task(task)

        # 2. Generate initial population
        population = []
        for i in range(self.population_size):
            code = self.programmer.generate_solver(task, analysis)
            fitness_result = calculate_fitness(code, task)
            population.append({
                "code": code,
                "fitness": fitness_result["fitness"],
                "train_correct": fitness_result["train_correct"],
                "test_correct": fitness_result["test_correct"]
            })

        # 3. Sort by fitness
        population.sort(key=lambda x: x["fitness"], reverse=True)

        return population
```

#### Cell 5: Load Test Data and Run Inference
```python
# Load private test set (provided by Kaggle)
with open("/kaggle/input/arc-prize-2025/arc-agi_test_challenges.json") as f:
    test_tasks = json.load(f)

submission = {}
evolution_engine = SimplifiedEvolution(population_size=5, max_generations=3)

for idx, (task_id, task) in enumerate(test_tasks.items()):
    print(f"Processing {idx+1}/{len(test_tasks)}: {task_id}")

    # Evolve population
    population = evolution_engine.evolve(task)

    # Select best 2 for pass@2
    best = population[0] if len(population) > 0 else None
    second = population[1] if len(population) > 1 else best

    # Generate predictions for all test inputs
    predictions = []
    for test_input in task.get("test", []):
        # Attempt 1 (best solver)
        success1, pred1, _ = execute_solver_safe(best["code"], np.array(test_input["input"]))
        attempt_1 = pred1.tolist() if success1 else [[0, 0], [0, 0]]

        # Attempt 2 (second-best solver)
        success2, pred2, _ = execute_solver_safe(second["code"], np.array(test_input["input"]))
        attempt_2 = pred2.tolist() if success2 else [[0, 0], [0, 0]]

        predictions.append({
            "attempt_1": attempt_1,
            "attempt_2": attempt_2
        })

    submission[task_id] = predictions

    # Progress update
    if (idx + 1) % 10 == 0:
        print(f"Completed {idx + 1}/{len(test_tasks)} tasks")
```

#### Cell 6: Save Submission
```python
with open("submission.json", "w") as f:
    json.dump(submission, f, indent=2)

print(f"Submission complete! Processed {len(submission)} tasks.")
```

**Deliverables:**
- Self-contained notebook (no external dependencies beyond model)
- All helper functions copied inline
- Simplified agents using local model
- Valid submission.json output

---

### Task 2: Model Preparation (2-3 hours)

**Download Code Gemma 7B and Upload to Kaggle:**

#### Step 1: Download locally
```bash
# On local machine with internet
python << EOF
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "google/codegemma-7b-it"
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Save to local directory
model.save_pretrained("./codegemma-7b/")
tokenizer.save_pretrained("./codegemma-7b/")
print("Model downloaded successfully!")
EOF
```

#### Step 2: Upload to Kaggle Dataset
1. Go to kaggle.com/datasets
2. Create new dataset: "codegemma-7b"
3. Upload files from `./codegemma-7b/`
4. Title: "Code Gemma 7B Instruct for ARC Prize"
5. Make public (optional) or keep private

**Model size:** ~15.93 GB (within Kaggle limits)

---

### Task 3: Local Testing (4-6 hours)

**Before uploading to Kaggle, test locally:**

#### Create test script: `scripts/test_kaggle_notebook_local.py`
```python
"""
Simulate Kaggle environment locally.
Test with evaluation set (not test set - we don't have test outputs).
"""
import json
import numpy as np

# Load evaluation set (has test outputs for validation)
with open("data/arc-prize-2025/arc-agi_evaluation_challenges_merged.json") as f:
    eval_tasks = json.load(f)

# Test on 10 tasks
sample_tasks = dict(list(eval_tasks.items())[:10])

# Run notebook cells programmatically
# ... (execute each cell, verify no errors)

print("Local testing complete!")
```

#### Validation checklist:
- [ ] No internet calls (all offline)
- [ ] Runs to completion on 10 tasks
- [ ] Average runtime < 3 min/task
- [ ] Valid submission.json format
- [ ] No crashes or timeouts

---

### Task 4: Kaggle Submission (2-3 hours)

**Upload and submit to Kaggle:**

1. **Upload notebook:** `notebooks/kaggle_submission.ipynb`
2. **Add dataset inputs:**
   - Code Gemma model (uploaded in Task 2)
   - ARC test data (auto-provided by competition)
3. **Test run:** Run on 5-10 tasks first (sample submission)
4. **Full submission:** Run on all 240 tasks
5. **Monitor:** Check logs, ensure no errors
6. **Submit:** Generate submission.json and submit to competition

---

## Key Differences from plan_20251031.md

| Aspect | plan_20251031.md ❌ | plan_20251101.md ✅ |
|--------|---------------------|---------------------|
| **Internet Access** | Assumed Gemini API available | NO internet - local models only |
| **Code Structure** | `from arc_prometheus...` | Self-contained notebook (no imports) |
| **Model** | Gemini API | Code Gemma 7B (local) |
| **Agents** | Full agents with API calls | Simplified offline agents |
| **Population Size** | 10-20 | 5 (time constraints) |
| **Generations** | 5-10 | 3 (time constraints) |
| **Dependencies** | External src/ code | All code inline in notebook |

---

## Success Criteria

**Task 1 Complete:**
- [ ] Notebook runs end-to-end locally
- [ ] Uses local model (no internet)
- [ ] Produces valid submission.json
- [ ] All helper functions work

**Task 2 Complete:**
- [ ] Model downloaded (15.93 GB)
- [ ] Uploaded to Kaggle dataset
- [ ] Model loads in notebook

**Task 3 Complete:**
- [ ] Tested on 10 evaluation tasks
- [ ] Average < 3 min/task
- [ ] No errors or crashes
- [ ] Submission format validated

**Task 4 Complete:**
- [ ] Successfully submitted to Kaggle
- [ ] Receives leaderboard score (any score validates workflow)
- [ ] Documentation updated with results

---

## Timeline

**Total: 3-4 days**
- Task 1 (Notebook creation): 6-8 hours
- Task 2 (Model prep): 2-3 hours
- Task 3 (Local testing): 4-6 hours
- Task 4 (Kaggle submission): 2-3 hours

**Remaining time after baseline:**
- Optimize (distillation, active inference)
- Improve score with better models/prompts
- Research analysis

---

## After Baseline Works: Optimizations (Phase 4b-4e)

Once we have a working baseline submission:

### Phase 4b: Knowledge Distillation (Optional, 3-4 days)
- Collect Gemini outputs on training set
- Fine-tune Code Gemma to mimic Gemini
- Expected: +5-10% improvement

### Phase 4c: Active Inference (Optional, 2-3 days)
- Augment training examples (3 → 30+)
- Fine-tune model per task at runtime
- Expected: +5-10% improvement

### Phase 4d: Multi-Agent Active Inference (Optional, 2-3 days)
- Each agent fine-tunes separately
- Expected: +2-5% over single-model

**Priority:** Get baseline working FIRST. Optimizations are secondary.

---

## Next Steps

1. Create `notebooks/kaggle_submission.ipynb` (Task 1)
2. Download Code Gemma 7B (Task 2)
3. Test locally on evaluation set (Task 3)
4. Submit to Kaggle (Task 4)

**Expected baseline score:** 10-20% (raw Code Gemma 7B, no fine-tuning)
**Competitive threshold:** 15-25% (validates AI Civilization concept)
**SOTA:** 34% (requires distillation + active inference)
